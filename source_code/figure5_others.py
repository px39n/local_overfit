"""
Figure 5 Supporting Functions

This module contains model fitting, evaluation, and visualization helper functions
for the accuracy surface analysis.
"""

import warnings
warnings.filterwarnings('ignore')

import numpy as np 
import pandas as pd
from tqdm.auto import tqdm

from sklearn.preprocessing import PolynomialFeatures, StandardScaler 
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.pipeline import Pipeline
from pygam import LinearGAM, s, l, f, te

# Optional: LightGBM (use fallback if not installed)
try:
    from lightgbm import LGBMRegressor
    HAS_LIGHTGBM = True
except ImportError:
    HAS_LIGHTGBM = False


# ============================================================
# Helper Functions (ported from OG_transformer.plot.aggregate_bin)
# ============================================================

def extract_model_names(df, lower=True):
    """Extract model names from predicted columns"""
    predicted_cols = [col for col in df.columns if 'predicted' in col.lower()]
    if lower:
        model_names = [col.lower().replace('predicted_', '') if 'predicted_' in col.lower() else 'default' for col in predicted_cols]
    else:
        model_names = [col.replace('predicted_', '') if 'predicted_' in col else 'default' for col in predicted_cols]
    return dict(zip(predicted_cols, model_names))


def calculate_score(observed, predicted, metric):
    """Calculate performance score"""
    if metric == 'r2':
        score = r2_score(observed, predicted)
        return max(0, score)  # clip negative R2 to 0
    elif metric == 'rmse':
        from sklearn.metrics import mean_squared_error
        return np.sqrt(mean_squared_error(observed, predicted))
    else:  # nrmse
        from sklearn.metrics import mean_squared_error
        return np.sqrt(mean_squared_error(observed, predicted)) / observed.std()


def calculate_r2_with_bins(df, sparsity_bins, use_seeds=True, verbose=False, target_model=None, sufficiency_bins=None, remove_outliers=False):
    """
    Calculate RÂ² data with binning.
    
    Args:
        df: DataFrame with predicted columns
        sparsity_bins: Number of sparsity bins
        use_seeds: Whether to aggregate by seeds
        verbose: Print debug information
        target_model: If specified, only calculate for this model
        sufficiency_bins: If specified, bin sufficiency values
        remove_outliers: Whether to remove outliers using IQR method
    
    Returns:
        DataFrame with binned RÂ² results
    """
    # Get model columns
    model_columns = [col for col in df.columns if col.startswith('predicted_')]
    
    # If target model specified, only process that model
    if target_model is not None:
        target_col = f'predicted_{target_model}'
        if target_col in model_columns:
            model_columns = [target_col]
        else:
            raise ValueError(f"Model {target_model} not found in data")
    
    # Create grouping variables
    df_copy = df.copy()
    sparsity_cut_result = pd.cut(df_copy['sparsity'], bins=sparsity_bins, labels=False, retbins=True)
    df_copy['sparsity_bin'] = sparsity_cut_result[0]
    actual_sparsity_bins = sparsity_cut_result[1]
    
    if verbose:
        sparsity_centers = [(actual_sparsity_bins[i] + actual_sparsity_bins[i+1])/2 for i in range(len(actual_sparsity_bins)-1)]
        print(f"Sparsity bin edges: {actual_sparsity_bins}")
        print(f"Sparsity bin centers: {sparsity_centers}")
    
    # Handle sufficiency binning
    unique_suff_values = np.sort(df_copy['sufficiency'].unique())
    n_unique = len(unique_suff_values)
    
    if sufficiency_bins is None:
        sufficiency_bins = n_unique
    
    values_per_bin = n_unique // sufficiency_bins
    remainder = n_unique % sufficiency_bins
    
    # Create bin mapping
    suff_to_bin = {}
    current_idx = 0
    
    for bin_idx in range(sufficiency_bins):
        current_bin_size = values_per_bin + (1 if bin_idx < remainder else 0)
        for i in range(current_bin_size):
            if current_idx < n_unique:
                suff_to_bin[unique_suff_values[current_idx]] = bin_idx
                current_idx += 1
    
    df_copy['sufficiency_bin'] = df_copy['sufficiency'].map(suff_to_bin)
    
    # Determine grouping
    base_group_cols = ['sufficiency_bin', 'sparsity_bin']
    
    if use_seeds and 'seed' in df_copy.columns:
        group_cols = base_group_cols + ['seed']
    else:
        group_cols = base_group_cols
        use_seeds = False
    
    # Calculate RÂ²
    results = []
    for name, group in df_copy.groupby(group_cols):
        if len(group) < 10:
            continue
            
        for model_col in model_columns:
            observed = group['observed'].values
            predicted = group[model_col].values
            valid_mask = ~(np.isnan(observed) | np.isnan(predicted) | 
                         np.isinf(observed) | np.isinf(predicted))
            if valid_mask.sum() < 5:
                continue
                
            r2 = r2_score(observed[valid_mask], predicted[valid_mask])
            
            result = {
                'sufficiency_bin': name[0],
                'sufficiency': group['sufficiency'].mean(),
                'sparsity_bin': name[1],
                'sparsity': group['sparsity'].mean(),
                'model': model_col.replace('predicted_', ''),
                'r2': r2,
                'n_samples': valid_mask.sum()
            }
            if use_seeds:
                result['seed'] = name[2]
            
            results.append(result)
    
    df_results = pd.DataFrame(results)
    
    # Aggregate if using seeds
    if use_seeds and len(df_results) > 0:
        agg_cols = ['sufficiency_bin', 'sparsity_bin', 'model']
        
        df_final = df_results.groupby(agg_cols).agg({
            'r2': ['mean', 'std', 'count'],
            'sufficiency': 'mean',
            'sparsity': 'mean',
            'n_samples': 'sum'
        }).reset_index()
        
        df_final.columns = ['sufficiency_bin', 'sparsity_bin', 'model', 'r2', 'r2_std', 'n_seeds', 'sufficiency', 'sparsity', 'total_samples']
        df_final['r2_std'] = df_final['r2_std'].fillna(0)
        df_final = df_final[['sufficiency_bin', 'sufficiency', 'sparsity_bin', 'sparsity', 'model', 'r2', 'r2_std', 'n_seeds', 'total_samples']]
    else:
        df_final = df_results.copy()
        df_final['r2_std'] = 0.0
        df_final['n_seeds'] = 1
        df_final['total_samples'] = df_final['n_samples']
        df_final = df_final[['sufficiency_bin', 'sufficiency', 'sparsity_bin', 'sparsity', 'model', 'r2', 'r2_std', 'n_seeds', 'total_samples']]
    
    if verbose:
        print(f"Generated {len(df_final)} data points for GAM fitting")
        if use_seeds:
            print(f"Average std across all points: {df_final['r2_std'].mean():.4f}")
    
    if remove_outliers:
        df_final_list = []
        total_removed = 0
        
        for model_name in df_final['model'].unique():
            model_data = df_final[df_final['model'] == model_name].copy()
            original_count = len(model_data)
            
            q1 = model_data['r2'].quantile(0.25)
            q3 = model_data['r2'].quantile(0.75)
            iqr = q3 - q1
            
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr
            
            model_data_clean = model_data[(model_data['r2'] >= lower_bound) & (model_data['r2'] <= upper_bound)]
            removed_count = original_count - len(model_data_clean)
            total_removed += removed_count
            
            if verbose and removed_count > 0:
                print(f"  {model_name}: Removed {removed_count}/{original_count} outliers (RÂ² < {lower_bound:.3f} or > {upper_bound:.3f})")
            
            df_final_list.append(model_data_clean)
        
        df_final = pd.concat(df_final_list, ignore_index=True)
        
        if verbose:
            print(f"Outlier removal: {total_removed} total points removed across all models")
    
    return df_final


def find_optimal_gam_params(df, sparsity_bins, target_model='Ours', use_seeds=True, 
                           lam_range=(0.1, 100), spline_range=(4, 20)):
    """
    Find optimal GAM hyperparameters using ternary search.
    
    Args:
        df: DataFrame
        sparsity_bins: Number of sparsity bins
        target_model: Target model name
        use_seeds: Whether to use seeds aggregation
        lam_range: Lambda parameter search range (min, max)
        spline_range: Spline parameter search range (min, max)
    
    Returns:
        tuple: (best_lam, best_spline, best_score)
    """
    import math
    
    target_col = f'predicted_{target_model}'
    if target_col not in df.columns:
        print(f"Error: Model {target_model} not found in data!")
        return 1.0, 8, -999
    
    other_predicted_cols = [col for col in df.columns if col.startswith('predicted_') and col != target_col]
    df_filtered = df.drop(columns=other_predicted_cols)
    
    df_bins = calculate_r2_with_bins(
        df_filtered, sparsity_bins, use_seeds, verbose=False, target_model=target_model
    )
    
    if len(df_bins) < 15:
        print("Error: Insufficient data for GAM fitting")
        return 1.0, 8, -999
    
    model_data = df_bins.copy()
    model_data['sufficiency_log'] = np.log10(model_data['sufficiency'])
    X = model_data[['sufficiency_log', 'sparsity']].values
    y = model_data['r2'].values
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    def evaluate_gam(lam, spline):
        try:
            gam_mono = LinearGAM(
                s(0, constraints='monotonic_inc', n_splines=spline, lam=lam) + 
                s(1, constraints='monotonic_inc', n_splines=spline, lam=lam)
            )
            gam_mono.fit(X_scaled, y)
            return gam_mono.statistics_['pseudo_r2']['explained_deviance']
        except:
            return -999
    
    # Stage 1: Search for optimal lam with fixed spline=8
    best_lam = 1.0
    best_score = -999
    
    log_lam_min, log_lam_max = math.log10(lam_range[0]), math.log10(lam_range[1])
    tested_lams = {}
    
    def get_lam_score(log_lam):
        if log_lam not in tested_lams:
            lam = 10 ** log_lam
            score = evaluate_gam(lam, 8)
            tested_lams[log_lam] = score
        return tested_lams[log_lam]
    
    left, right = log_lam_min, log_lam_max
    while right - left > 0.2:
        mid1 = left + (right - left) / 3
        mid2 = right - (right - left) / 3
        
        score1 = get_lam_score(mid1)
        score2 = get_lam_score(mid2)
        
        if score1 > best_score:
            best_score = score1
            best_lam = 10 ** mid1
        if score2 > best_score:
            best_score = score2
            best_lam = 10 ** mid2
        
        if score1 < score2:
            left = mid1
        else:
            right = mid2
    
    # Stage 2: Search for optimal spline with best lam
    best_spline = 8
    tested_splines = {}
    
    def get_spline_score(spline):
        if spline not in tested_splines:
            score = evaluate_gam(best_lam, spline)
            tested_splines[spline] = score
        return tested_splines[spline]
    
    left, right = spline_range[0], spline_range[1]
    while right - left > 2:
        mid1 = left + (right - left) // 3
        mid2 = right - (right - left) // 3
        
        score1 = get_spline_score(mid1)
        score2 = get_spline_score(mid2)
        
        if score1 > best_score:
            best_score = score1
            best_spline = mid1
        if score2 > best_score:
            best_score = score2
            best_spline = mid2
        
        if score1 < score2:
            left = mid1
        else:
            right = mid2
    
    for spline in range(left, right + 1):
        score = get_spline_score(spline)
        if score > best_score:
            best_score = score
            best_spline = spline
    
    # Stage 3: Fine-tune lam with best spline
    current_log_lam = math.log10(best_lam)
    search_range = 0.5
    
    left = max(log_lam_min, current_log_lam - search_range)
    right = min(log_lam_max, current_log_lam + search_range)
    
    while right - left > 0.1:
        mid1 = left + (right - left) / 3
        mid2 = right - (right - left) / 3
        
        lam1, lam2 = 10 ** mid1, 10 ** mid2
        score1 = evaluate_gam(lam1, best_spline)
        score2 = evaluate_gam(lam2, best_spline)
        
        if score1 > best_score:
            best_score = score1
            best_lam = lam1
        if score2 > best_score:
            best_score = score2
            best_lam = lam2
        
        if score1 < score2:
            left = mid1
        else:
            right = mid2
    
    return best_lam, best_spline, best_score


def find_optimal_sparsity_bins(df, min_bins=5, max_bins=25, use_seeds=True, target_model='Ours',
                              lam=1.0, spline=8):
    """
    Find optimal sparsity bin count using ternary search.
    
    Args:
        df: DataFrame
        min_bins: Minimum bin count
        max_bins: Maximum bin count
        use_seeds: Whether to use seeds aggregation
        target_model: Target model name
        lam: GAM lambda parameter
        spline: GAM spline parameter
    
    Returns:
        int: Optimal bin count
    """
    target_col = f'predicted_{target_model}'
    if target_col not in df.columns:
        print(f"Error: Model {target_model} not found in data!")
        available_models = [col.replace('predicted_', '') for col in df.columns if col.startswith('predicted_')]
        print(f"Available models: {available_models}")
        return min_bins
    
    other_predicted_cols = [col for col in df.columns if col.startswith('predicted_') and col != target_col]
    df_filtered = df.drop(columns=other_predicted_cols)
    
    def evaluate_bins(n_bins):
        try:
            # Use local fit_gam_models instead of importing from OG_transformer
            fitted_models = fit_gam_models(
                df_filtered, n_bins, use_seeds, spline=spline, lam=lam, verbose=False
            )
            
            if target_model not in fitted_models:
                return -999
            
            model_info = fitted_models[target_model]
            r2_val = model_info['r2_score']
            
            return r2_val
                
        except Exception as e:
            return -999
    
    left, right = min_bins, max_bins
    best_bins, best_score = min_bins, -999
    
    tested_points = {}
    
    def get_score(bins):
        if bins not in tested_points:
            score = evaluate_bins(bins)
            tested_points[bins] = score
        return tested_points[bins]
    
    # Ternary search
    while right - left > 2:
        mid1 = left + (right - left) // 3
        mid2 = right - (right - left) // 3
        
        score1 = get_score(mid1)
        score2 = get_score(mid2)
        
        if score1 > best_score:
            best_score = score1
            best_bins = mid1
        if score2 > best_score:
            best_score = score2
            best_bins = mid2
        
        if score1 < score2:
            left = mid1
        else:
            right = mid2
    
    # Check remaining points
    for bins in range(left, right + 1):
        score = get_score(bins)
        if score > best_score:
            best_score = score
            best_bins = bins
    
    print(f"GAM parameters: lam={lam:.3f}, spline={spline}")
    print(f"Optimal bins: {best_bins} (RÂ²={best_score:.4f})")
    
    return best_bins

def fit_accuracy_surface(results_dict, model_type='svm', poly_degree=2):
    """
    å¯¹å‡†ç¡®ç‡ç»“æœè¿›è¡Œè¡¨é¢æ‹Ÿåˆ
    
    Args:
        results_dict: åŒ…å«sufficiency, sparsity, accuracyæ•°æ®çš„å­—å…¸
        model_type: ä½¿ç”¨çš„æ¨¡å‹ç±»å‹ ('svm', 'lightgbm', 'linear', 'gam_free')
        poly_degree: å¤šé¡¹å¼ç‰¹å¾çš„æ¬¡æ•°(ä»…ç”¨äºéƒ¨åˆ†æ¨¡å‹)
        
    Returns:
        dict: åŒ…å«æ‹Ÿåˆæ¨¡å‹å’Œç›¸å…³ä¿¡æ¯çš„å­—å…¸
    """
    # æå–æ•°æ®
    sufficiency = np.log10(results_dict['sufficiency'])  # å–å¯¹æ•°
    sparsity = results_dict['sparsity']
    accuracy = results_dict['accuracy']
    
    # å‡†å¤‡ç‰¹å¾çŸ©é˜µ
    X = np.column_stack([sufficiency, sparsity])
    y = accuracy
    
    # æ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # æ ¹æ®æ¨¡å‹ç±»å‹è¿›è¡Œæ‹Ÿåˆ
    r2_test = _fit_single_model(model_type, X_train, X_test, y_train, y_test)
    
    # å‡†å¤‡é¢„æµ‹ç½‘æ ¼
    suff_range = np.linspace(sufficiency.min(), sufficiency.max(), 50)
    spar_range = np.linspace(sparsity.min(), sparsity.max(), 50)
    suff_grid, spar_grid = np.meshgrid(suff_range, spar_range)
    
    # åˆ›å»ºæœ€ç»ˆæ¨¡å‹ç”¨äºé¢„æµ‹
    if model_type == 'svm':
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        model = SVR(kernel='rbf', C=1.0, gamma='scale')
        model.fit(X_scaled, y)
        
        # é¢„æµ‹
        grid_points = np.column_stack([suff_grid.ravel(), spar_grid.ravel()])
        grid_points_scaled = scaler.transform(grid_points)
        predictions = model.predict(grid_points_scaled)
        
    elif model_type == 'gam_free':
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        model = LinearGAM(s(0, n_splines=8, lam=1) + s(1, n_splines=8, lam=1))
        model.fit(X_scaled, y)
        
        # é¢„æµ‹
        grid_points = np.column_stack([suff_grid.ravel(), spar_grid.ravel()])
        grid_points_scaled = scaler.transform(grid_points)
        predictions = model.predict(grid_points_scaled)
        
    else:  # linear, lightgbmç­‰
        if model_type == 'linear':
            model = LinearRegression()
        elif model_type == 'lightgbm' and HAS_LIGHTGBM:
            model = LGBMRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, 
                                random_state=42, verbose=-1)
        else:
            model = LinearRegression()  # é»˜è®¤ä½¿ç”¨çº¿æ€§å›å½’
            
        model.fit(X, y)
        
        # é¢„æµ‹
        grid_points = np.column_stack([suff_grid.ravel(), spar_grid.ravel()])
        predictions = model.predict(grid_points)
    
    # é‡å¡‘é¢„æµ‹ç»“æœ
    predictions = predictions.reshape(suff_grid.shape)
    
    return {
        'model': model,
        'scaler': scaler if model_type in ['svm', 'gam_free'] else None,
        'predictions': predictions,
        'suff_grid': suff_grid,
        'spar_grid': spar_grid,
        'r2_test': r2_test,
        'model_type': model_type
    }

def fit_gam_models(df, sparsity_bins, use_seeds=True, sufficiency_bins=None, spline=14, lam=5, verbose=True, remove_outliers=True, gam_type='spline'):
    """
    é‡æ„çš„GAMæ¨¡å‹æ‹Ÿåˆå‡½æ•°
    
    Args:
        df: åŒ…å«è§‚æµ‹å€¼å’Œé¢„æµ‹å€¼çš„DataFrame
        sparsity_bins: sparsityåˆ†binçš„è¾¹ç•Œ
        use_seeds: æ˜¯å¦æŒ‰seedsåˆ†ç»„è®¡ç®—ï¼ˆTrueï¼‰è¿˜æ˜¯ç›´æ¥è®¡ç®—å…¨éƒ¨ï¼ˆFalseï¼‰
        spline: GAMæ ·æ¡å‡½æ•°çš„èŠ‚ç‚¹æ•°
        lam: GAMæ­£åˆ™åŒ–å‚æ•°
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        remove_outliers: æ˜¯å¦ç§»é™¤RÂ²å¼‚å¸¸å€¼ä»¥æé«˜æ‹Ÿåˆè´¨é‡
        gam_type: GAMå‡½æ•°ç±»å‹ ('spline', 'linear', 'mixed', 'polynomial', 'monotonic_polynomial')
        
    Returns:
        dict: æ‹Ÿåˆå¥½çš„æ¨¡å‹å­—å…¸
    """
    # 1. ä½¿ç”¨ç‹¬ç«‹å‡½æ•°è®¡ç®—RÂ²æ•°æ®
    df_bins = calculate_r2_with_bins(
        df=df, 
        sparsity_bins=sparsity_bins,
        use_seeds=use_seeds,
        verbose=verbose,
        remove_outliers=remove_outliers,
        sufficiency_bins=sufficiency_bins
    )
    
    # 2. ä¸ºæ¯ä¸ªæ¨¡å‹æ‹ŸåˆGAM
    fitted_models = {}
    
    for model_name in df_bins['model'].unique():
        model_data = df_bins[df_bins['model'] == model_name].copy()
        
        # ç‰¹å¾å·¥ç¨‹
        model_data['sufficiency_log'] = np.log10(model_data['sufficiency'])
        X = model_data[['sufficiency_log', 'sparsity']].values
        y = model_data['r2'].values
        
        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # ç‰¹æ®Šå¤„ç† monotonic_polynomialï¼ˆç§»å‡ºå¤–å±‚try-exceptï¼‰
        if gam_type == 'monotonic_polynomial':
            try:
                # å•è°ƒ + äº¤äº’æ•ˆåº”ï¼šå•è°ƒæ ·æ¡ + æ— çº¦æŸäº¤äº’é¡¹
                gam_model = LinearGAM(
                    s(0, constraints='monotonic_inc', n_splines=spline, lam=lam) + 
                    s(1, constraints='monotonic_inc', n_splines=spline, lam=lam) +
                    te(0, 1, n_splines=[max(4, spline//4), max(4, spline//4)], lam=lam*2)
                )
                
                # æ‹Ÿåˆæ¨¡å‹
                gam_model.fit(X_scaled, y)
                
                # ä¿å­˜æ¨¡å‹ä¿¡æ¯ï¼ˆæ ‡å‡†æ ¼å¼ï¼‰
                fitted_models[model_name] = {
                    'model': gam_model,
                    'scaler': scaler,
                    'r2_score': gam_model.statistics_['pseudo_r2']['explained_deviance'],
                    'gam_type': gam_type
                }
                
                if verbose:
                    print(f"Model {model_name} ({gam_type}): RÂ² = {gam_model.statistics_['pseudo_r2']['explained_deviance']:.4f}, n_points = {len(model_data)}")
                    
            except Exception as poly_error:
                if verbose:
                    print(f"Model {model_name} ({gam_type}): Failed to fit - {str(poly_error)}")
            continue
        
        # æ ¹æ®gam_typeé€‰æ‹©ä¸åŒçš„GAMæ¨¡å‹
        try:
            if gam_type == 'spline':
                # å•è°ƒæ ·æ¡å‡½æ•° (é»˜è®¤)
                gam_model = LinearGAM(
                    s(0, constraints='monotonic_inc', n_splines=spline, lam=lam) + 
                    s(1, constraints='monotonic_inc', n_splines=spline, lam=lam)
                )
                
            elif gam_type == 'linear':
                # çº¯çº¿æ€§é¡¹ (å¤©ç„¶å•è°ƒ)
                gam_model = LinearGAM(
                    l(0, lam=lam) + l(1, lam=lam)
                )
                
            elif gam_type == 'mixed':
                # æ··åˆï¼šsufficiencyç”¨æ ·æ¡ï¼Œsparsityç”¨çº¿æ€§
                gam_model = LinearGAM(
                    s(0, constraints='monotonic_inc', n_splines=spline, lam=lam) + 
                    l(1, lam=lam)
                )
                
            elif gam_type == 'polynomial':
                # å¤šé¡¹å¼ï¼šä½¿ç”¨tensor productæ¥è¿‘ä¼¼å¤šé¡¹å¼äº¤äº’æ•ˆæœï¼Œæ— çº¦æŸ
                gam_model = LinearGAM(
                    s(0, n_splines=spline, lam=lam) + 
                    s(1, n_splines=spline, lam=lam) + 
                    te(0, 1, n_splines=[spline//2, spline//2], lam=lam)
                )
                

            else:
                raise ValueError(f"Unknown gam_type: {gam_type}")
            
            # æ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ç»Ÿä¸€çš„æ‹Ÿåˆæ–¹å¼
            gam_model.fit(X_scaled, y)
            r2_val = gam_model.statistics_['pseudo_r2']['explained_deviance']
            
            # ä¿å­˜æ¨¡å‹ä¿¡æ¯
            fitted_models[model_name] = {
                'model': gam_model,
                'scaler': scaler,
                'r2_score': r2_val,
                'gam_type': gam_type
            }
            
            # æ ¹æ®verboseå‚æ•°å†³å®šæ˜¯å¦æ‰“å°ä¿¡æ¯
            if verbose:
                print(f"Model {model_name} ({gam_type}): RÂ² = {r2_val:.4f}, n_points = {len(model_data)}")
                
        except Exception as e:
            if verbose:
                print(f"Model {model_name} ({gam_type}): Failed to fit - {str(e)}")
            continue
    
    return fitted_models

def fit_universal_models(df, sparsity_bins, model_type='gam_monotonic', use_seeds=True, sufficiency_bins=None, 
                        spline=7, lam=0.5, verbose=True, remove_outliers=True, full_features='Density', resolution=None, splitby='grid', outlier_threshold=-9999, diagnose=False, spline_order=2, diagnose_path=None):
    """
    é€šç”¨æ¨¡å‹æ‹Ÿåˆå‡½æ•° - æ”¯æŒæ‰€æœ‰_fit_single_modelä¸­çš„æ¨¡å‹ç±»å‹
    ç±»ä¼¼fit_gam_modelsï¼Œä¸€æ¬¡åªæ‹Ÿåˆä¸€ä¸ªæ¨¡å‹ç±»å‹
    
    Args:
        df: åŒ…å«è§‚æµ‹å€¼å’Œé¢„æµ‹å€¼çš„DataFrame
        sparsity_bins: sparsityåˆ†binçš„è¾¹ç•Œ
        model_type: æ¨¡å‹ç±»å‹ï¼Œæ”¯æŒ:
                    'linear', 'ridge', 'elasticnet', 'rf', 'gbr', 'lightgbm', 'svm',
                    'svm_with_constraint' (SVM with light monotonic constraints on density/sufficiency),
                    'gam_free', 'gam_monotonic', 'gam_without_interaction', 'gam_mono_noint',
                    'interpolation' (IDW spatial interpolation, requires spatial features),
                    'two_stage' (GAM Density + SVM Spatial Residual, requires Full features),
                    'two_stage_1' (Only Stage 1: GAM Density prediction, requires Density features),
                    'two_stage_2' (Only Stage 2: SVM fitted residual prediction)
                    Note: 'two_stage_residual' not supported here (requires y_true)
        use_seeds: æ˜¯å¦æŒ‰seedsåˆ†ç»„è®¡ç®—ï¼ˆTrueï¼‰è¿˜æ˜¯ç›´æ¥è®¡ç®—å…¨éƒ¨ï¼ˆFalseï¼‰
        sufficiency_bins: sufficiencyåˆ†binçš„æ•°é‡
        spline: GAMæ ·æ¡å‡½æ•°çš„èŠ‚ç‚¹æ•°
        lam: GAMæ­£åˆ™åŒ–å‚æ•°
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        remove_outliers: æ˜¯å¦ç§»é™¤RÂ²å¼‚å¸¸å€¼ä»¥æé«˜æ‹Ÿåˆè´¨é‡
        full_features: str or list
                      String options:
                        'Density': ä½¿ç”¨[sufficiency_log, sparsity]
                        'Spatial': ä½¿ç”¨[longitude, latitude] (éœ€è¦dfä¸­æœ‰longitude, latitudeåˆ—)
                        'Full': ä½¿ç”¨å…¨éƒ¨4ä¸ªç‰¹å¾[longitude, latitude, sufficiency_log, sparsity]
                      List: ç›´æ¥æŒ‡å®šç‰¹å¾åˆ—è¡¨ï¼Œä¾‹å¦‚ ['longitude', 'latitude', 'sparsity']
                            ä¼šä»Fullæ¨¡å¼çš„èšåˆæ•°æ®ä¸­é€‰æ‹©è¿™äº›ç‰¹å¾
        resolution: [lon_bins, lat_bins], é»˜è®¤[10, 10] - ç”¨äºspatial aggregation
        splitby: 'grid' or 'station' - èšåˆæ–¹å¼
        outlier_threshold: RÂ² å¼‚å¸¸å€¼å¤„ç†ï¼ˆé»˜è®¤ -9999ï¼Œå‡ ä¹ä¸è¿‡æ»¤ï¼‰
                          - æ•°å­—ï¼ˆå¦‚ -0.5ï¼‰ï¼šå°† RÂ² <= threshold çš„å€¼æ›¿æ¢ä¸º threshold
                          - å­—ç¬¦ä¸²ï¼ˆå¦‚ "0_remove", "-0.5_remove"ï¼‰ï¼šç§»é™¤ RÂ² <= threshold çš„æ•°æ®ç‚¹
                          - Noneï¼šä¸è¿›è¡Œä»»ä½•å¤„ç†
        diagnose: æ˜¯å¦åœ¨è®­ç»ƒ two_stage æ¨¡å‹åç»˜åˆ¶è¯Šæ–­å›¾ï¼ˆé»˜è®¤ Falseï¼‰
                 ä»…å¯¹ two_stage/two_stage_1/two_stage_2 æ¨¡å‹æœ‰æ•ˆ
        spline_order: GAM æ ·æ¡é˜¶æ•°ï¼ˆé»˜è®¤ 3 = ä¸‰æ¬¡æ ·æ¡ï¼‰
                     0=å¸¸æ•°, 1=çº¿æ€§, 2=äºŒæ¬¡, 3=ä¸‰æ¬¡
                     é™ä½å¯ç”¨æ›´å°‘æ ·æœ¬ï¼Œä½†æ›²çº¿æ›´"ç¡¬"
                     ä»…å¯¹ GAM å’Œ two_stage ç³»åˆ—æ¨¡å‹æœ‰æ•ˆ
        diagnose_path: dict, è¯Šæ–­å›¾ä¿å­˜è·¯å¾„ï¼ˆä»…å¯¹ two_stage ç³»åˆ—æœ‰æ•ˆï¼‰
                      æ ¼å¼ï¼š{"model_name": ["stage1_path.svg", "stage2_path.png"]}
                      ä¾‹å¦‚ï¼š{"mlp+biglightgbm": ["figures/stage1.svg", "figures/stage2.png"]}
                      åªæœ‰åœ¨å­—å…¸ä¸­æŒ‡å®šçš„æ¨¡å‹æ‰ä¼šä¿å­˜è¯Šæ–­å›¾
                      æ ¹æ®æ–‡ä»¶æ‰©å±•åè‡ªåŠ¨è¯†åˆ«æ ¼å¼ï¼ˆsvg/png/pdfç­‰ï¼‰
                      None åˆ™ä¸ä¿å­˜
        
    Returns:
        dict: æ‹Ÿåˆå¥½çš„æ¨¡å‹å­—å…¸ï¼Œæ ¼å¼ä¸º {model_name: {...}}ï¼Œä¸fit_gam_modelsç›¸åŒ
              æ¯ä¸ªæ¨¡å‹åŒ…å«: 'model', 'scaler', 'r2_score', 'model_type', 'full_features'
    
    Examples:
        >>> # ä½¿ç”¨é¢„è®¾ç‰¹å¾é›†
        >>> models = fit_universal_models(df, sparsity_bins=7, full_features='Full')
        
        >>> # ä½¿ç”¨è‡ªå®šä¹‰ç‰¹å¾åˆ—è¡¨
        >>> models = fit_universal_models(
        ...     df, sparsity_bins=7, 
        ...     full_features=['longitude', 'latitude', 'sparsity']
        ... )
        
        >>> # åªä½¿ç”¨ç©ºé—´ç‰¹å¾
        >>> models = fit_universal_models(
        ...     df, sparsity_bins=7,
        ...     full_features=['longitude', 'latitude']
        ... )
    """
    if resolution is None:
        resolution = [10, 10]
    
    # Check if full_features is a list (custom features)
    is_custom_features = isinstance(full_features, (list, tuple))
    
    # 1. æ ¹æ®full_featureså‡†å¤‡æ•°æ®
    if is_custom_features:
        # Custom feature list - use Full mode aggregation then select specified features
        if verbose:
            print(f"Using custom features: {full_features}")
            print(f"Aggregating by spatial grid with sampling features (resolution={resolution})...")
        
        # Validate that required features are available
        required_cols = set(full_features)
        spatial_cols = {'longitude', 'latitude'}
        needs_spatial = bool(spatial_cols & required_cols)
        
        if needs_spatial and ('longitude' not in df.columns or 'latitude' not in df.columns):
            raise ValueError("Custom features include longitude/latitude but these columns are not in df")
        
        model_columns = [col for col in df.columns if col.startswith('predicted_')]
        model_names = [col.replace('predicted_', '') for col in model_columns]
        
        bins_intervals = find_bins_intervals(df, sparsity_bins)
        aggregated_data = prepare_spatial_features(
            df, model_names, split_by=splitby,
            include_sampling_features=True,  # Need all features to select from
            bins_intervals=bins_intervals,
            resolution=resolution,
            outlier_threshold=outlier_threshold
        )
        
    elif full_features == 'Spatial':
        # ä½¿ç”¨spatial featuresè¿›è¡Œaggregation
        if 'longitude' not in df.columns or 'latitude' not in df.columns:
            raise ValueError("Spatial mode requires 'longitude' and 'latitude' columns in df")
        
        if verbose:
            print(f"Aggregating by spatial grid (resolution={resolution})...")
        
        # è·å–æ¨¡å‹åˆ—è¡¨
        model_columns = [col for col in df.columns if col.startswith('predicted_')]
        model_names = [col.replace('predicted_', '') for col in model_columns]
        
        # ä½¿ç”¨prepare_spatial_featuresè¿›è¡Œèšåˆ
        bins_intervals = find_bins_intervals(df, sparsity_bins)
        aggregated_data = prepare_spatial_features(
            df, model_names, split_by='grid',
            include_sampling_features=False,  # Spatialæ¨¡å¼ä¸åŒ…å«sampling features
            bins_intervals=None,
            resolution=resolution,
            outlier_threshold=outlier_threshold
        )
        
    elif full_features == 'Full':
        # ä½¿ç”¨spatial + sampling features
        if 'longitude' not in df.columns or 'latitude' not in df.columns:
            raise ValueError("Full mode requires 'longitude' and 'latitude' columns in df")
        
        if verbose:
            print(f"Aggregating by spatial grid with sampling features (resolution={resolution})...")
        
        model_columns = [col for col in df.columns if col.startswith('predicted_')]
        model_names = [col.replace('predicted_', '') for col in model_columns]
        
        bins_intervals = find_bins_intervals(df, sparsity_bins)
        aggregated_data = prepare_spatial_features(
            df, model_names, split_by=splitby,
            include_sampling_features=True,  # Fullæ¨¡å¼åŒ…å«æ‰€æœ‰features
            bins_intervals=bins_intervals,
            resolution=resolution,
            outlier_threshold=outlier_threshold
        )
        
    else:  # Density (default)
        # ä½¿ç”¨åŸæœ‰çš„calculate_r2_with_binsæ–¹æ³•
        if verbose:
            print(f"Aggregating by sufficiencyÃ—sparsity bins...")
        
        df_bins = calculate_r2_with_bins(
            df=df, 
            sparsity_bins=sparsity_bins,
            use_seeds=use_seeds,
            verbose=verbose,
            remove_outliers=remove_outliers,
            sufficiency_bins=sufficiency_bins
        )
        
        # è½¬æ¢ä¸ºaggregated_dataæ ¼å¼
        aggregated_data = {}
        for model_name in df_bins['model'].unique():
            model_data = df_bins[df_bins['model'] == model_name].copy()
            model_data['sufficiency_log'] = np.log10(model_data['sufficiency'])
            aggregated_data[model_name] = model_data[['sufficiency_log', 'sparsity', 'r2']]
    
    # 2. ä¸ºæ¯ä¸ªæ¨¡å‹æ‹ŸåˆæŒ‡å®šçš„model_type
    fitted_models = {}
    
    # ç‰¹æ®Šå¤„ç†ï¼štwo_stage ç³»åˆ—éœ€è¦åŸå§‹æ•°æ®
    # two_stage_residual éœ€è¦ç‰¹æ®Šå¤„ç†ï¼ˆä¸èƒ½ç”¨äºç½‘æ ¼é¢„æµ‹ï¼Œåªèƒ½ç”¨äºè¯„ä¼°ï¼‰
    if model_type == 'two_stage_residual':
        raise ValueError("two_stage_residual cannot be used in fit_universal_models (requires y_true). "
                        "Use two_stage_2 to predict fitted SVM residuals, or use eval_baseline_comparison for residual analysis.")
    
    if model_type in ['two_stage', 'two_stage_1', 'two_stage_2']:
        if model_type in ['two_stage', 'two_stage_2']:
            # two_stage å’Œ two_stage_2 éœ€è¦å…¨éƒ¨ 4 ä¸ªç‰¹å¾
            if full_features != 'Full' and not (is_custom_features and 
                                               set(full_features) >= {'longitude', 'latitude', 'sufficiency_log', 'sparsity'}):
                raise ValueError(f"{model_type} model requires full_features='Full' or custom features with all 4 columns")
        # two_stage_1 å¯ä»¥ä½¿ç”¨ Density ç‰¹å¾æˆ–æ›´å¤š
        
        # è·å–æ¨¡å‹åˆ—è¡¨
        model_columns = [col for col in df.columns if col.startswith('predicted_')]
        model_names = [col.replace('predicted_', '') for col in model_columns]
        
        # å‡†å¤‡ bins_intervals
        bins_intervals = find_bins_intervals(df, sparsity_bins)
        
        # ä¸ºæ¯ä¸ªæ¨¡å‹è®­ç»ƒ two_stage
        for model_name in model_names:
            # ä» diagnose_path å­—å…¸ä¸­è·å–å½“å‰æ¨¡å‹çš„è·¯å¾„ï¼ˆå¦‚æœæŒ‡å®šï¼‰
            model_diagnose_path = diagnose_path.get(model_name) if diagnose_path else None
            
            two_stage_model = TwoStageModel(
                spline=spline, lam=lam, resolution=resolution, 
                diagnose=diagnose, spline_order=spline_order,
                diagnose_path=model_diagnose_path
            )
            stage1_score, stage2_score = two_stage_model.fit(
                df, model_name, bins_intervals,
                use_seeds=use_seeds, split_by=splitby, metric='r2'
            )
            
            # æ ¹æ® model_type é€‰æ‹©æ˜¾ç¤ºçš„å¾—åˆ†
            if model_type == 'two_stage_1':
                display_score = stage1_score
            elif model_type == 'two_stage_2':
                display_score = stage2_score  # Stage2 æœ¬èº«ä¸å¥½è¯„ä¼°ï¼Œè¿™é‡Œä¿æŒ stage2
            else:  # two_stage
                display_score = stage2_score
            
            fitted_models[model_name] = {
                'model': two_stage_model,
                'scaler': None,
                'r2_score': display_score,
                'model_type': model_type,
                'full_features': full_features,
                'stage1_score': stage1_score,
                'stage2_score': stage2_score
            }
            
            if verbose:
                if model_type == 'two_stage_1':
                    print(f"  {model_name} [two_stage_1]: Stage1={stage1_score:.4f} (only), n_points = {len(df)}")
                elif model_type == 'two_stage_2':
                    print(f"  {model_name} [two_stage_2]: Stage2 residual (Stage1={stage1_score:.4f}, Stage2={stage2_score:.4f}), n_points = {len(df)}")
                else:
                    print(f"  {model_name} [two_stage]: Stage1={stage1_score:.4f}, Stage2={stage2_score:.4f}, n_points = {len(df)}")
        
        return fitted_models
    
    # æ™®é€šæ¨¡å‹è®­ç»ƒæµç¨‹
    for model_name in aggregated_data.keys():
        model_data = aggregated_data[model_name].copy()
        
        if len(model_data) < 10:
            if verbose:
                print(f"Skipping {model_name}: insufficient data (n={len(model_data)})")
            continue
        
        # å‡†å¤‡ç‰¹å¾
        if is_custom_features:
            # Custom feature list
            try:
                X = model_data[list(full_features)].values
            except KeyError as e:
                if verbose:
                    print(f"  {model_name}: Missing feature {e}, skipping")
                continue
        elif full_features == 'Full':
            X = model_data[['longitude', 'latitude', 'sufficiency_log', 'sparsity']].values
        elif full_features == 'Spatial':
            X = model_data[['longitude', 'latitude']].values
        else:  # Density
            X = model_data[['sufficiency_log', 'sparsity']].values
        
        y = model_data['r2'].values
        
        # ç‰¹æ®Šå¤„ç†ï¼šinterpolation éœ€è¦ spatial features
        if model_type == 'interpolation':
            if full_features == 'Spatial' or (is_custom_features and set(full_features) >= {'longitude', 'latitude'}):
                # ç¡®ä¿ä½¿ç”¨ lon/lat
                X = model_data[['longitude', 'latitude']].values
            else:
                if verbose:
                    print(f"  {model_name} [interpolation]: Requires spatial features (longitude, latitude), skipping")
                continue
        
        try:
            # ä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒï¼ˆæ²¡æœ‰train/test splitï¼Œç±»ä¼¼fit_gam_modelsï¼‰
            score, model_obj, scaler = _fit_single_model(
                model_type, X, X, y, y, 
                metric='r2', spline=spline, lam=lam, return_model=True
            )
            
            if score == -999:
                if verbose:
                    print(f"  {model_name} [{model_type}]: Failed to fit")
                continue
            
            # ä¿å­˜æ¨¡å‹ä¿¡æ¯ï¼ˆä¸fit_gam_modelsæ ¼å¼ç›¸åŒï¼‰
            fitted_models[model_name] = {
                'model': model_obj,
                'scaler': scaler,
                'r2_score': score,
                'model_type': model_type,
                'full_features': full_features
            }
            
            if verbose:
                print(f"  {model_name} [{model_type}]: RÂ² = {score:.4f}, n_points = {len(model_data)}")
                
        except Exception as e:
            if verbose:
                print(f"  {model_name} [{model_type}]: Failed - {str(e)}")
            continue
    
    return fitted_models


class IDWInterpolator:
    """IDW (Inverse Distance Weighting) æ’å€¼å™¨ï¼Œæä¾›ç»Ÿä¸€çš„ .predict() æ¥å£"""
    
    def __init__(self, X_train, y_train):
        """
        Args:
            X_train: è®­ç»ƒæ•°æ®åæ ‡ (n_samples, 2) - [longitude, latitude]
            y_train: è®­ç»ƒæ•°æ®å€¼ (n_samples,)
        """
        self.X_train = X_train
        self.y_train = y_train
    
    def predict(self, X_test):
        """
        ä½¿ç”¨ IDW æ–¹æ³•é¢„æµ‹
        
        Args:
            X_test: æµ‹è¯•æ•°æ®åæ ‡ (n_samples, 2)
            
        Returns:
            y_pred: é¢„æµ‹å€¼ (n_samples,)
        """
        y_pred = []
        for x_test_point in X_test:
            distances = np.sqrt(np.sum((self.X_train - x_test_point)**2, axis=1))
            
            # é¿å…é™¤é›¶ï¼šå¦‚æœè·ç¦»ä¸º0ï¼Œç›´æ¥ä½¿ç”¨è¯¥ç‚¹çš„å€¼
            if np.any(distances == 0):
                y_pred.append(self.y_train[distances == 0][0])
            else:
                # IDWæ’å€¼ï¼šæƒé‡ = 1 / distance^2
                weights = 1.0 / (distances ** 2)
                weights = weights / np.sum(weights)  # å½’ä¸€åŒ–
                y_pred.append(np.sum(weights * self.y_train))
        
        return np.array(y_pred)


class TwoStageModel:
    """
    åŒé˜¶æ®µæ¨¡å‹ï¼šGAM (Density) + SVM (Spatial Residual)
    
    Stage 1: ä½¿ç”¨ sampling aggregation + GAM_Monotonic é¢„æµ‹åŸºç¡€ RÂ² (åŸºäº density ç‰¹å¾)
            ğŸ” è‡ªåŠ¨æ£€æµ‹ sufficiency æ•°é‡ï¼š
            - å•ä¸€ sufficiency â†’ ä½¿ç”¨ sparsity å•å˜é‡å•è°ƒ GAM
            - å¤šä¸ª sufficiency â†’ ä½¿ç”¨ (sufficiency_log, sparsity) åŒå˜é‡ GAM + äº¤äº’é¡¹
    
    Stage 2: ä½¿ç”¨ spatial aggregation + SVM é¢„æµ‹æ®‹å·® (åŸºäº spatial ç‰¹å¾)
    
    Final: é¢„æµ‹å€¼ = GAM é¢„æµ‹ + SVM æ®‹å·®é¢„æµ‹
    """
    
    def __init__(self, spline=7, lam=0.5, resolution=None, stage2_features=None, diagnose=False, spline_order=2, svm_only=0, clip=None, diagnose_path=None):
        """
        Args:
            spline: GAM æ ·æ¡å‡½æ•°èŠ‚ç‚¹æ•°
            lam: GAM æ­£åˆ™åŒ–å‚æ•°
            resolution: spatial aggregation åˆ†è¾¨ç‡ [lon_bins, lat_bins]
            stage2_features: ç¬¬äºŒé˜¶æ®µ SVM ä½¿ç”¨çš„ç‰¹å¾åˆ—è¡¨
                           é»˜è®¤ ['longitude', 'latitude']
                           å¯é€‰æ·»åŠ  'sufficiency_log' å’Œ/æˆ– 'sparsity'
                           ä¾‹å¦‚ï¼š['longitude', 'latitude', 'sufficiency_log', 'sparsity']
            diagnose: æ˜¯å¦åœ¨è®­ç»ƒåç»˜åˆ¶è¯Šæ–­å›¾ï¼ˆé»˜è®¤ Falseï¼‰
            spline_order: æ ·æ¡é˜¶æ•°ï¼ˆ0=å¸¸æ•°, 1=çº¿æ€§, 2=äºŒæ¬¡, 3=ä¸‰æ¬¡[é»˜è®¤]ï¼‰
                         é™ä½ spline_order å¯ä»¥ç”¨æ›´å°‘çš„æ ·æœ¬ï¼Œä½†æ›²çº¿ä¼šæ›´"ç¡¬"
            svm_only: æ˜¯å¦åªä½¿ç”¨ SVMï¼ˆè·³è¿‡ GAM Stage1ï¼‰ï¼Œé»˜è®¤ False
            clip: RÂ² æˆªæ–­èŒƒå›´ [min, max]ï¼Œé»˜è®¤ [-0.5, 1.0]
                 åœ¨æ•°æ®å‡†å¤‡é˜¶æ®µæˆªæ–­å¼‚å¸¸ RÂ² å€¼ï¼Œé˜²æ­¢æç«¯å€¼å½±å“è®­ç»ƒ
                 è®¾ä¸º None åˆ™ä¸æˆªæ–­
            diagnose_path: è¯Šæ–­å›¾ä¿å­˜è·¯å¾„ [stage1_path, stage2_path]
                          ä¾‹å¦‚ï¼š["figures/stage1.svg", "figures/stage2.png"]
                          æ ¹æ®æ–‡ä»¶æ‰©å±•åè‡ªåŠ¨è¯†åˆ«æ ¼å¼ï¼ŒNone åˆ™ä¸ä¿å­˜
        """
        self.spline = spline
        self.lam = lam
        self.resolution = resolution if resolution else [10, 10]
        self.stage2_features = stage2_features if stage2_features else ['longitude', 'latitude', 'sufficiency_log']# ['longitude', 'latitude',]#['longitude', 'latitude']   'sparsity'
        self.diagnose = diagnose
        self.spline_order = spline_order
        self.svm_only = svm_only
        self.clip = clip if clip is not None else [-0.5, 1.0]  # é»˜è®¤æˆªæ–­èŒƒå›´
        self.diagnose_path = diagnose_path  # è¯Šæ–­å›¾ä¿å­˜è·¯å¾„
        
        # ç¬¬ä¸€é˜¶æ®µï¼šGAM æ¨¡å‹ (Density features)
        self.gam_model = None
        self.gam_scaler = None
        
        # ç¬¬äºŒé˜¶æ®µï¼šSVM æ¨¡å‹ (Spatial features for residual)
        self.svm_model = None
        self.svm_scaler = None
        
        # è‡ªåŠ¨æ£€æµ‹æ ‡å¿—ï¼šæ˜¯å¦åªæœ‰å•ä¸€ sufficiency å€¼
        self.single_sufficiency = False
        
        # è¯Šæ–­æ•°æ®ç¼“å­˜ - Stage 1 (GAM)
        self._stage1_X_raw = None
        self._stage1_X_scaled = None
        self._stage1_y_true = None
        self._stage1_y_pred = None
        self._actual_spline = None  # å®é™…ä½¿ç”¨çš„ spline æ•°é‡ï¼ˆå¯èƒ½è‡ªåŠ¨è°ƒæ•´ï¼‰
        
        # è¯Šæ–­æ•°æ®ç¼“å­˜ - Stage 2 (SVM)
        self._stage2_data = None  # å®Œæ•´çš„ stage2 è®­ç»ƒæ•°æ® DataFrame
    
    def fit(self, df_train_raw, model_name, bins_intervals, use_seeds=True, split_by='grid', metric='r2'):
        """
        è®­ç»ƒåŒé˜¶æ®µæ¨¡å‹
        
        Args:
            df_train_raw: åŸå§‹è®­ç»ƒæ•°æ®
            model_name: è¦é¢„æµ‹çš„æ¨¡å‹åç§°ï¼ˆä¾‹å¦‚ 'Ours'ï¼‰
            bins_intervals: sparsity bins åŒºé—´
            use_seeds: æ˜¯å¦ä½¿ç”¨ seeds èšåˆ
            split_by: spatial èšåˆæ–¹å¼ ('grid' or 'station')
            metric: è¯„ä¼°æŒ‡æ ‡ ('r2' or 'correlation')
        
        Returns:
            stage1_score: ç¬¬ä¸€é˜¶æ®µ GAM çš„å¾—åˆ†
            stage2_score: ç¬¬äºŒé˜¶æ®µï¼ˆGAM + SVMï¼‰çš„å¾—åˆ†
        """
        from sklearn.metrics import r2_score
        from scipy.stats import pearsonr
        from sklearn.preprocessing import StandardScaler
        from sklearn.svm import SVR
        from pygam import LinearGAM, s, te
        
        # ====== Stage 1: ä½¿ç”¨ sampling aggregation è®­ç»ƒ GAM ======
        # stage1_data_dict = prepare_sampling_features(
        #     bins_intervals, df_train_raw, [model_name], use_seeds,
        #     include_spatial_coords=True
        # )

        stage1_data_dict = prepare_sampling_features_space(
                bins_intervals, df_train_raw, [model_name], 
                split_by=split_by,
                resolution=self.resolution,
                use_seeds=use_seeds,
                clip=self.clip  # ä¼ é€’æˆªæ–­å‚æ•°
            )

        stage1_data = stage1_data_dict.get(model_name)
        
        if stage1_data is None or len(stage1_data) < 5:
            raise ValueError(f"Insufficient data for model {model_name} in stage 1")
        
        # ğŸ” è‡ªåŠ¨æ£€æµ‹ï¼šsufficiency æ˜¯å¦åªæœ‰å”¯ä¸€å€¼
        # ä½¿ç”¨å®¹å·®åˆ¤æ–­ï¼Œé¿å…æµ®ç‚¹æ•°ç²¾åº¦é—®é¢˜å¯¼è‡´çš„å‡é˜³æ€§
        suff_log_values = stage1_data['sufficiency_log'].values
        suff_log_std = np.std(suff_log_values)
        self.single_sufficiency = (suff_log_std < 1e-6)  # æ ‡å‡†å·® < 1e-6 è§†ä¸ºå•ä¸€å€¼
        self.stage2_features=['longitude', 'latitude']
        
        # æå– density ç‰¹å¾ï¼ˆæ ¹æ® sufficiency æ•°é‡é€‰æ‹©ï¼‰
        if self.single_sufficiency:
            # åªæœ‰å•ä¸€ sufficiencyï¼Œåªç”¨ sparsity
            X_stage1 = stage1_data[['sparsity']].values
            # print(f"  âš ï¸  Detected single sufficiency value, using sparsity-only monotonic GAM")
        else:
            # å¤šä¸ª sufficiencyï¼Œä½¿ç”¨åŒå˜é‡ GAM
            X_stage1 = stage1_data[['sufficiency_log', 'sparsity']].values
        
        y_stage1 = stage1_data['r2'].values
        
        # ğŸ” è‡ªåŠ¨è°ƒæ•´ spline æ•°é‡ï¼šæ»¡è¶³ PyGAM è¦æ±‚ n_splines > spline_order
        n_samples = len(y_stage1)
        min_splines = self.spline_order + 1
        adjusted_spline = self.spline
        self._actual_spline = adjusted_spline
        # è®­ç»ƒ GAM with monotonic constraints
        self.gam_scaler = StandardScaler()
        X_stage1_scaled = self.gam_scaler.fit_transform(X_stage1)
        
        if self.single_sufficiency:
            # å•å˜é‡ GAMï¼šåªç”¨ sparsity (monotonic increasing)
            self.gam_model = LinearGAM(
                s(0, constraints='monotonic_inc', n_splines=adjusted_spline, 
                  spline_order=self.spline_order, lam=self.lam)
            )
        else:
            # åŒå˜é‡ GAMï¼šsufficiency_log + sparsity + interaction
            interaction_spline = max(min_splines, adjusted_spline // 4)  # æ¢å¤åŸå§‹é€»è¾‘
            self.gam_model = LinearGAM(
                s(0, constraints='monotonic_inc', n_splines=adjusted_spline, 
                  spline_order=self.spline_order, lam=self.lam) + 
                s(1, constraints='monotonic_inc', n_splines=adjusted_spline, 
                  spline_order=self.spline_order, lam=self.lam) +
                te(0, 1, n_splines=[interaction_spline, interaction_spline],spline_order=self.spline_order,  lam=self.lam*2)
            )
        
        self.gam_model.fit(X_stage1_scaled, y_stage1)
        
        # è¯„ä¼° Stage 1
        y_gam_pred_train = self.gam_model.predict(X_stage1_scaled)
        if metric == 'correlation':
            stage1_score, _ = pearsonr(y_stage1, y_gam_pred_train)
        else:
            stage1_score = r2_score(y_stage1, y_gam_pred_train)
        
        # ğŸ” ä¿å­˜è¯Šæ–­æ•°æ®å¹¶å¯é€‰ç»˜å›¾
        self._stage1_X_raw = X_stage1  # åŸå§‹ç‰¹å¾ï¼ˆæœªæ ‡å‡†åŒ–ï¼‰
        self._stage1_X_scaled = X_stage1_scaled  # æ ‡å‡†åŒ–åçš„ç‰¹å¾
        self._stage1_y_true = y_stage1
        self._stage1_y_pred = y_gam_pred_train
        
        if self.diagnose:
            stage1_save_path = self.diagnose_path[0] if self.diagnose_path and len(self.diagnose_path) > 0 else None
            self.plot_stage1_diagnosis(save_path=stage1_save_path)
        
        # ====== Stage 2: ä½¿ç”¨ spatial aggregation è®­ç»ƒ SVMï¼ˆé¢„æµ‹æ®‹å·®ï¼‰======
        stage2_data_dict = prepare_spatial_features(
            df_train_raw, [model_name],
            split_by=split_by,
            include_sampling_features=True,  # éœ€è¦ density ç‰¹å¾ç”¨äº GAM é¢„æµ‹
            bins_intervals=bins_intervals,
            resolution=self.resolution,
            clip=self.clip  # ä¼ é€’æˆªæ–­å‚æ•°
        )
        stage2_data = stage2_data_dict.get(model_name)
        
        if stage2_data is None or len(stage2_data) < 5:
            raise ValueError(f"Insufficient spatial data for model {model_name} in stage 2")
        
        # è®¡ç®—æ®‹å·®
        y_true = stage2_data['r2'].values
        
        if self.svm_only:
            # SVM-only æ¨¡å¼ï¼šç›´æ¥æ‹Ÿåˆ RÂ²ï¼Œä¸ç”¨ GAM
            residuals = y_true
            y_gam_pred = 0  # å ä½ï¼Œä¸ä½¿ç”¨
        else:
            # æ­£å¸¸æ¨¡å¼ï¼šä½¿ç”¨ GAM é¢„æµ‹ stage2 æ•°æ®ï¼ˆæ ¹æ® sufficiency æ•°é‡é€‰æ‹©ç‰¹å¾ï¼‰
            if self.single_sufficiency:
                X_stage2_density = stage2_data[['sparsity']].values
            else:
                X_stage2_density = stage2_data[['sufficiency_log', 'sparsity']].values
            
            X_stage2_density_scaled = self.gam_scaler.transform(X_stage2_density)
            y_gam_pred = self.gam_model.predict(X_stage2_density_scaled)
            residuals = y_true - y_gam_pred
        
        # è®­ç»ƒ SVM é¢„æµ‹æ®‹å·®ï¼ˆä½¿ç”¨å¯é…ç½®çš„ç‰¹å¾ï¼‰
        X_stage2_spatial = stage2_data[self.stage2_features].values
        
        self.svm_scaler = StandardScaler()
        X_stage2_spatial_scaled = self.svm_scaler.fit_transform(X_stage2_spatial)
        
        self.svm_model = SVR(kernel='rbf', C=1, gamma='scale')
        self.svm_model.fit(X_stage2_spatial_scaled, residuals)
        
        # è¯„ä¼° Stage 2 (GAM + SVM)
        residual_pred = self.svm_model.predict(X_stage2_spatial_scaled)
        y_final_pred = y_gam_pred + residual_pred
        
        if metric == 'correlation':
            stage2_score, _ = pearsonr(y_true, y_final_pred)
        else:
            stage2_score = r2_score(y_true, y_final_pred)
        
        # ğŸ” ä¿å­˜ Stage 2 è¯Šæ–­æ•°æ®
        stage2_data_diag = stage2_data.copy()
        stage2_data_diag['r2_gam_pred'] = y_gam_pred if not self.svm_only else 0
        stage2_data_diag['residual_true'] = residuals
        stage2_data_diag['residual_pred'] = residual_pred
        self._stage2_data = stage2_data_diag
        
        # ğŸ” è‡ªåŠ¨è¯Šæ–­ Stage 2ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.diagnose:
            stage2_save_path = self.diagnose_path[1] if self.diagnose_path and len(self.diagnose_path) > 1 else None
            self.plot_stage2_diagnosis(save_path=stage2_save_path)
        
        return stage1_score, stage2_score
    
    def plot_stage1_diagnosis(self, figsize=None, save_path=None):
        """
        è¯Šæ–­ Stage 1 GAM æ‹Ÿåˆæ•ˆæœ
        
        å• sufficiency: 1Ã—3 å›¾ï¼ˆsparsity çš„ 3 ä¸ªè§†è§’ï¼‰
        å¤š sufficiency: 2Ã—3 å›¾
          - ç¬¬ä¸€è¡Œï¼šsufficiency_log çš„ 3 ä¸ªè§†è§’
          - ç¬¬äºŒè¡Œï¼šsparsity çš„ 3 ä¸ªè§†è§’
        
        Args:
            figsize: å›¾ç‰‡å¤§å°ï¼Œé»˜è®¤è‡ªåŠ¨ï¼ˆå•å˜é‡ 14Ã—5ï¼ŒåŒå˜é‡ 14Ã—10ï¼‰
            save_path: ä¿å­˜è·¯å¾„ï¼Œæ ¹æ®æ‰©å±•åè‡ªåŠ¨è¯†åˆ«æ ¼å¼ï¼ˆsvg/pngï¼‰ï¼ŒNone åˆ™ä¸ä¿å­˜
        """
        import matplotlib.pyplot as plt
        import numpy as np
        
        if self._stage1_X_raw is None or self._stage1_y_true is None:
            print("âš ï¸  No diagnosis data available. Run fit() first.")
            return
        
        X_raw = self._stage1_X_raw
        y_true = self._stage1_y_true
        y_pred = self._stage1_y_pred
        
        # æ ¹æ®æ¨¡å¼ç¡®å®šå¸ƒå±€
        if self.single_sufficiency:
            # å•å˜é‡ï¼š1Ã—3 å›¾
            nrows, ncols = 1, 3
            figsize = figsize or (14, 5)
        else:
            # åŒå˜é‡ï¼š2Ã—3 å›¾
            nrows, ncols = 2, 3
            figsize = figsize or (14, 10)
        
        fig, axes = plt.subplots(nrows, ncols, figsize=figsize)
        
        # ç¡®ä¿ axes æ˜¯äºŒç»´æ•°ç»„
        if nrows == 1:
            axes = axes.reshape(1, -1)
        
        # ç»˜åˆ¶å‡½æ•°ï¼šä¸ºæ¯ä¸ªå˜é‡ç”» 3 ä¸ªå­å›¾
        def plot_variable_row(ax_row, X_var, var_name, row_idx):
            """ç”»ä¸€è¡Œï¼ˆ3ä¸ªå­å›¾ï¼‰ï¼šX vs y_true, X vs y_pred+curve, y_true vs y_pred"""
            
            # å­å›¾ 1: X vs y_true
            ax_row[0].scatter(X_var, y_true, alpha=0.6, s=30, c='steelblue', edgecolors='k', linewidths=0.5)
            ax_row[0].set_xlabel(var_name, fontsize=12)
            ax_row[0].set_ylabel('RÂ² (True)', fontsize=12)
            ax_row[0].set_title(f'{var_name}: Training Data', fontsize=12, fontweight='bold')
            ax_row[0].grid(True, alpha=0.3)
            
            # å­å›¾ 2: X vs y_pred + GAM æ‹Ÿåˆæ›²çº¿
            ax_row[1].scatter(X_var, y_pred, alpha=0.6, s=30, c='coral', edgecolors='k', linewidths=0.5, label='GAM predictions')
            
            # ç”Ÿæˆæ‹Ÿåˆæ›²çº¿
            X_range = np.linspace(X_var.min(), X_var.max(), 200)
            
            if self.single_sufficiency:
                # å•å˜é‡ï¼šç›´æ¥é¢„æµ‹
                X_range_input = X_range.reshape(-1, 1)
                X_range_scaled = self.gam_scaler.transform(X_range_input)
                y_curve = self.gam_model.predict(X_range_scaled)
            else:
                # åŒå˜é‡ï¼šè®¡ç®—è¾¹é™…å¹³å‡æ•ˆåº”ï¼ˆå¯¹å¦ä¸€ä¸ªå˜é‡çš„æ‰€æœ‰å€¼å–å¹³å‡ï¼‰
                if row_idx == 0:
                    # ç¬¬ä¸€è¡Œï¼ˆsufficiency_logï¼‰ï¼šå¯¹æ‰€æœ‰ sparsity å€¼å–å¹³å‡
                    other_var_values = X_raw[:, 1]  # æ‰€æœ‰ sparsity å€¼
                    y_curve_list = []
                    for x_val in X_range:
                        # å¯¹æ¯ä¸ª sufficiency_log å€¼ï¼Œéå†æ‰€æœ‰ sparsity
                        X_grid = np.column_stack([
                            np.full(len(other_var_values), x_val),  # å›ºå®š sufficiency_log
                            other_var_values  # æ‰€æœ‰ sparsity
                        ])
                        X_grid_scaled = self.gam_scaler.transform(X_grid)
                        y_grid = self.gam_model.predict(X_grid_scaled)
                        y_curve_list.append(y_grid.mean())  # å¹³å‡
                    y_curve = np.array(y_curve_list)
                else:
                    # ç¬¬äºŒè¡Œï¼ˆsparsityï¼‰ï¼šå¯¹æ‰€æœ‰ sufficiency_log å€¼å–å¹³å‡
                    other_var_values = X_raw[:, 0]  # æ‰€æœ‰ sufficiency_log å€¼
                    y_curve_list = []
                    for x_val in X_range:
                        # å¯¹æ¯ä¸ª sparsity å€¼ï¼Œéå†æ‰€æœ‰ sufficiency_log
                        X_grid = np.column_stack([
                            other_var_values,  # æ‰€æœ‰ sufficiency_log
                            np.full(len(other_var_values), x_val)  # å›ºå®š sparsity
                        ])
                        X_grid_scaled = self.gam_scaler.transform(X_grid)
                        y_grid = self.gam_model.predict(X_grid_scaled)
                        y_curve_list.append(y_grid.mean())  # å¹³å‡
                    y_curve = np.array(y_curve_list)
            ax_row[1].plot(X_range, y_curve, 'b-', linewidth=2.5, label='GAM curve', alpha=0.8)
            
            ax_row[1].set_xlabel(var_name, fontsize=12)
            ax_row[1].set_ylabel('RÂ² (GAM Predicted)', fontsize=12)
            ax_row[1].set_title(f'{var_name}: GAM Predictions + Curve', fontsize=12, fontweight='bold')
            ax_row[1].legend(loc='best', fontsize=9)
            ax_row[1].grid(True, alpha=0.3)
            
            # å­å›¾ 3: y_true vs y_predï¼ˆæ‰€æœ‰è¡Œå…±äº«ï¼‰
            ax_row[2].scatter(y_true, y_pred, alpha=0.6, s=30, c='green', edgecolors='k', linewidths=0.5)
            ax_row[2].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 
                         'r--', linewidth=2, label='Perfect fit')
            ax_row[2].set_xlabel('RÂ² (True)', fontsize=12)
            ax_row[2].set_ylabel('RÂ² (GAM Predicted)', fontsize=12)
            ax_row[2].set_title('GAM Fit Quality', fontsize=12, fontweight='bold')
            ax_row[2].legend(fontsize=9)
            ax_row[2].grid(True, alpha=0.3)
        
        # æ ¹æ®æ¨¡å¼ç”»å›¾
        if self.single_sufficiency:
            # å•å˜é‡ï¼šåªç”» sparsity
            plot_variable_row(axes[0], X_raw[:, 0], 'Sparsity', 0)
        else:
            # åŒå˜é‡ï¼šç¬¬ä¸€è¡Œ sufficiency_logï¼Œç¬¬äºŒè¡Œ sparsity
            plot_variable_row(axes[0], X_raw[:, 0], 'Sufficiency_log', 0)
            plot_variable_row(axes[1], X_raw[:, 1], 'Sparsity', 1)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        from sklearn.metrics import r2_score, mean_absolute_error
        r2 = r2_score(y_true, y_pred)
        mae = mean_absolute_error(y_true, y_pred)
        
        # æ·»åŠ æ ‡æ³¨
        mode_text = "Single Sufficiency (Sparsity-only GAM)" if self.single_sufficiency else "Multiple Sufficiency (Full GAM)"
        fig.suptitle(f'Stage 1 GAM Diagnosis | Mode: {mode_text} | RÂ²={r2:.4f}, MAE={mae:.4f}', 
                     fontsize=14, fontweight='bold', y=1.02)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡ï¼ˆå¦‚æœæŒ‡å®šäº†è·¯å¾„ï¼‰
        if save_path:
            import os
            os.makedirs(os.path.dirname(save_path), exist_ok=True) if os.path.dirname(save_path) else None
            file_format = os.path.splitext(save_path)[1][1:]  # è·å–æ‰©å±•åï¼ˆä¸å«ç‚¹ï¼‰
            plt.savefig(save_path, format=file_format, dpi=300, bbox_inches='tight')
            print(f"âœ“ Stage 1 diagram saved to: {save_path}")
        
        plt.show()
        
        # æ‰“å°è¯¦ç»†ç»Ÿè®¡
        print(f"\n{'='*70}")
        print(f"Stage 1 GAM Training Diagnosis Report")
        print(f"{'='*70}")
        print(f"Mode: {mode_text}")
        print(f"Training samples: {len(y_true)}")
        if self._actual_spline is not None:
            spline_text = f"{self._actual_spline}"
            if self._actual_spline < self.spline:
                spline_text += f" (auto-adjusted from {self.spline})"
            print(f"Splines used: {spline_text}")
        print(f"")
        print(f"Training Feature Range (Raw):")
        for i in range(X_raw.shape[1]):
            feat_name = 'sparsity' if self.single_sufficiency else ('sufficiency_log' if i == 0 else 'sparsity')
            print(f"  {feat_name:20s}: [{X_raw[:, i].min():.6e}, {X_raw[:, i].max():.6e}]")
        print(f"")
        print(f"Model Performance:")
        print(f"  RÂ² score: {r2:.6f}")
        print(f"  MAE: {mae:.6f}")
        print(f"  Prediction range: [{y_pred.min():.6f}, {y_pred.max():.6f}]")
        print(f"  True RÂ² range: [{y_true.min():.6f}, {y_true.max():.6f}]")
        
        if abs(y_pred.max() - y_pred.min()) < 1e-6:
            print(f"\nâš ï¸  WARNING: GAM predictions are nearly constant!")
            print(f"   This suggests the model failed to learn from the data.")
            print(f"   Possible causes:")
            print(f"   - Too few training samples ({len(y_true)} points)")
            print(f"   - Feature range too small")
            print(f"   - StandardScaler issue")
            print(f"   - GAM hyperparameters (lam={self.lam})")
        
        print(f"{'='*70}")
        print(f"ğŸ’¡ Use model.diagnose_prediction(X_test) to check prediction issues")
        print(f"{'='*70}\n")
    
    def diagnose_prediction(self, X_test):
        """
        è¯Šæ–­é¢„æµ‹æ—¶çš„ç‰¹å¾å¤„ç†
        
        æ£€æŸ¥ï¼š
        1. è¾“å…¥ç‰¹å¾çš„èŒƒå›´
        2. æ ‡å‡†åŒ–åçš„ç‰¹å¾èŒƒå›´
        3. ä¸è®­ç»ƒæ•°æ®çš„å¯¹æ¯”
        
        Args:
            X_test: æµ‹è¯•æ•°æ® [longitude, latitude, sufficiency_log, sparsity]
        """
        import numpy as np
        
        if self.gam_model is None:
            print("âš ï¸  Model not fitted yet.")
            return
        
        print(f"\n{'='*70}")
        print(f"Stage 1 GAM Prediction Diagnosis")
        print(f"{'='*70}")
        
        # æå–ç‰¹å¾
        feature_map = {
            'sufficiency_log': 2,
            'sparsity': 3
        }
        
        if self.single_sufficiency:
            X_density = X_test[:, feature_map['sparsity']].reshape(-1, 1)
            feature_names = ['sparsity']
        else:
            X_density = np.column_stack([
                X_test[:, feature_map['sufficiency_log']],
                X_test[:, feature_map['sparsity']]
            ])
            feature_names = ['sufficiency_log', 'sparsity']
        
        print(f"\n1ï¸âƒ£  Mode: {'Single Sufficiency (sparsity-only)' if self.single_sufficiency else 'Multiple Sufficiency'}")
        print(f"   Features used: {feature_names}")
        print(f"   Test samples: {len(X_test)}")
        
        # æ˜¾ç¤ºåŸå§‹ç‰¹å¾èŒƒå›´
        print(f"\n2ï¸âƒ£  Test Data (Raw Features):")
        for i, name in enumerate(feature_names):
            print(f"   {name:20s}: [{X_density[:, i].min():.6e}, {X_density[:, i].max():.6e}]")
            print(f"   {'':20s}  Mean={X_density[:, i].mean():.6e}, Std={X_density[:, i].std():.6e}")
        
        # æ ‡å‡†åŒ–
        X_density_scaled = self.gam_scaler.transform(X_density)
        
        print(f"\n3ï¸âƒ£  Test Data (After Scaling):")
        for i, name in enumerate(feature_names):
            print(f"   {name:20s}: [{X_density_scaled[:, i].min():.6e}, {X_density_scaled[:, i].max():.6e}]")
            print(f"   {'':20s}  Mean={X_density_scaled[:, i].mean():.6e}, Std={X_density_scaled[:, i].std():.6e}")
        
        # ä¸è®­ç»ƒæ•°æ®å¯¹æ¯”
        if self._stage1_X_raw is not None:
            print(f"\n4ï¸âƒ£  Training Data Reference (Raw Features):")
            for i, name in enumerate(feature_names):
                print(f"   {name:20s}: [{self._stage1_X_raw[:, i].min():.6e}, {self._stage1_X_raw[:, i].max():.6e}]")
                print(f"   {'':20s}  Mean={self._stage1_X_raw[:, i].mean():.6e}, Std={self._stage1_X_raw[:, i].std():.6e}")
            
            print(f"\n5ï¸âƒ£  Training Data Reference (After Scaling):")
            for i, name in enumerate(feature_names):
                print(f"   {name:20s}: [{self._stage1_X_scaled[:, i].min():.6e}, {self._stage1_X_scaled[:, i].max():.6e}]")
                print(f"   {'':20s}  Mean={self._stage1_X_scaled[:, i].mean():.6e}, Std={self._stage1_X_scaled[:, i].std():.6e}")
        
        # é¢„æµ‹
        y_pred = self.gam_model.predict(X_density_scaled)
        
        print(f"\n6ï¸âƒ£  GAM Predictions:")
        print(f"   Range: [{y_pred.min():.6e}, {y_pred.max():.6e}]")
        print(f"   Mean={y_pred.mean():.6e}, Std={y_pred.std():.6e}")
        
        if abs(y_pred.max() - y_pred.min()) < 1e-6:
            print(f"\n   âš ï¸  WARNING: Predictions are nearly constant!")
        
        # æ£€æŸ¥ Scaler å‚æ•°
        print(f"\n7ï¸âƒ£  StandardScaler Parameters:")
        print(f"   Mean (used for scaling): {self.gam_scaler.mean_}")
        print(f"   Std  (used for scaling): {self.gam_scaler.scale_}")
        
        print(f"{'='*70}\n")
        
        return X_density, X_density_scaled, y_pred
    
    def plot_stage2_diagnosis(self, figsize=None, save_path=None):
        """
        è¯Šæ–­ Stage 2 SVM ç©ºé—´æ®‹å·®æ‹Ÿåˆæ•ˆæœ
        
        æŒ‰ä¸åŒ sufficiency åˆ†è¡Œï¼Œæ¯è¡Œ 4 åˆ—ï¼š
        - ç¬¬1åˆ—ï¼šåŸå§‹ RÂ² ç©ºé—´åˆ†å¸ƒï¼ˆè®­ç»ƒæ•°æ®ï¼‰
        - ç¬¬2åˆ—ï¼šæ®‹å·®ç©ºé—´åˆ†å¸ƒï¼ˆçœŸå®æ®‹å·®ï¼Œå¾…é¢„æµ‹ï¼‰
        - ç¬¬3åˆ—ï¼šSVM é¢„æµ‹çš„æ®‹å·®ç©ºé—´åˆ†å¸ƒ
        - ç¬¬4åˆ—ï¼šçœŸå®æ®‹å·® vs é¢„æµ‹æ®‹å·®æ•£ç‚¹å›¾
        
        Args:
            figsize: å›¾ç‰‡å¤§å°ï¼Œé»˜è®¤è‡ªåŠ¨ï¼ˆæ¯è¡Œé«˜åº¦ 4ï¼‰
            save_path: ä¿å­˜è·¯å¾„ï¼Œæ ¹æ®æ‰©å±•åè‡ªåŠ¨è¯†åˆ«æ ¼å¼ï¼ˆsvg/pngï¼‰ï¼ŒNone åˆ™ä¸ä¿å­˜
        """
        import matplotlib.pyplot as plt
        from sklearn.metrics import r2_score, mean_absolute_error
        
        if self._stage2_data is None:
            print("âš ï¸  No Stage 2 diagnosis data available. Run fit() first.")
            return
        
        df = self._stage2_data
        
        # è·å–å”¯ä¸€çš„ sufficiency_bin å€¼ï¼ˆå¦‚æœæœ‰ï¼‰
        if 'sufficiency_bin' in df.columns:
            unique_bins = sorted(df['sufficiency_bin'].unique())
            n_rows = len(unique_bins)
        else:
            # å¦‚æœæ²¡æœ‰ sufficiency_binï¼Œç”¨å•è¡Œ
            unique_bins = [None]
            n_rows = 1
        
        # è®¾ç½®å›¾ç‰‡å¤§å°
        if figsize is None:
            figsize = (16, 4 * n_rows)
        
        fig, axes = plt.subplots(n_rows, 4, figsize=figsize)
        if n_rows == 1:
            axes = axes.reshape(1, -1)  # ç¡®ä¿ axes æ˜¯ 2D
        
        for row_idx, suff_bin in enumerate(unique_bins):
            # ç­›é€‰å½“å‰ sufficiency_bin çš„æ•°æ®
            if suff_bin is not None:
                df_subset = df[df['sufficiency_bin'] == suff_bin]
                suff_mean = 10 ** df_subset['sufficiency_log'].mean()
                title_suffix = f"Sufficiency â‰ˆ {suff_mean:.0f}"
            else:
                df_subset = df
                title_suffix = "All Data"
            
            lon = df_subset['longitude'].values
            lat = df_subset['latitude'].values
            r2_true = df_subset['r2'].values
            residual_true = df_subset['residual_true'].values
            residual_pred = df_subset['residual_pred'].values
            
            # è®¡ç®—æ®‹å·®é¢„æµ‹æ€§èƒ½
            residual_r2 = r2_score(residual_true, residual_pred)
            residual_mae = mean_absolute_error(residual_true, residual_pred)
            
            # è®¡ç®—æ®‹å·®çš„ç»Ÿä¸€é¢œè‰²èŒƒå›´ï¼ˆç¬¬2ã€3åˆ—ä½¿ç”¨ç›¸åŒèŒƒå›´ä»¥ä¾¿å¯¹æ¯”ï¼‰
            residual_min = min(residual_true.min(), residual_pred.min())
            residual_max = max(residual_true.max(), residual_pred.max())
            # å¯¹ç§°åŒ–èŒƒå›´ï¼ˆä½¿0åœ¨ä¸­å¿ƒï¼‰
            residual_abs_max = max(abs(residual_min), abs(residual_max))
            residual_vmin = -residual_abs_max
            residual_vmax = residual_abs_max
            
            # ç¬¬1åˆ—ï¼šåŸå§‹ RÂ² ç©ºé—´åˆ†å¸ƒ
            ax1 = axes[row_idx, 0]
            scatter1 = ax1.scatter(lon, lat, c=r2_true, cmap='viridis', s=40, alpha=0.7, edgecolors='k', linewidths=0.5)
            ax1.set_xlabel('Longitude', fontsize=10)
            ax1.set_ylabel('Latitude', fontsize=10)
            ax1.set_title(f'Original RÂ²\n{title_suffix}', fontsize=11, fontweight='bold')
            ax1.grid(True, alpha=0.3)
            cbar1 = plt.colorbar(scatter1, ax=ax1)
            cbar1.set_label('RÂ² (True)', fontsize=9)
            ax1.text(0.02, 0.98, f'n={len(df_subset)}', transform=ax1.transAxes,
                    verticalalignment='top', fontsize=8, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
            
            # ç¬¬2åˆ—ï¼šçœŸå®æ®‹å·®ç©ºé—´åˆ†å¸ƒ
            ax2 = axes[row_idx, 1]
            scatter2 = ax2.scatter(lon, lat, c=residual_true, cmap='coolwarm', s=40, alpha=0.7, edgecolors='k', linewidths=0.5, vmin=residual_vmin, vmax=residual_vmax)
            ax2.set_xlabel('Longitude', fontsize=10)
            ax2.set_ylabel('Latitude', fontsize=10)
            ax2.set_title(f'Residual (True)\nRÂ² - GAM pred', fontsize=11, fontweight='bold')
            ax2.grid(True, alpha=0.3)
            cbar2 = plt.colorbar(scatter2, ax=ax2)
            cbar2.set_label('Residual', fontsize=9)
            ax2.text(0.02, 0.98, f'Range: [{residual_true.min():.3f}, {residual_true.max():.3f}]', 
                    transform=ax2.transAxes, verticalalignment='top', fontsize=8,
                    bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))
            
            # ç¬¬3åˆ—ï¼šSVM é¢„æµ‹çš„æ®‹å·®ç©ºé—´åˆ†å¸ƒ
            ax3 = axes[row_idx, 2]
            scatter3 = ax3.scatter(lon, lat, c=residual_pred, cmap='coolwarm', s=40, alpha=0.7, edgecolors='k', linewidths=0.5, vmin=residual_vmin, vmax=residual_vmax)
            ax3.set_xlabel('Longitude', fontsize=10)
            ax3.set_ylabel('Latitude', fontsize=10)
            ax3.set_title(f'Residual (SVM pred)\nRÂ²={residual_r2:.3f}', fontsize=11, fontweight='bold')
            ax3.grid(True, alpha=0.3)
            cbar3 = plt.colorbar(scatter3, ax=ax3)
            cbar3.set_label('Residual (pred)', fontsize=9)
            ax3.text(0.02, 0.98, f'MAE={residual_mae:.3f}', 
                    transform=ax3.transAxes, verticalalignment='top', fontsize=8,
                    bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))
            
            # ç¬¬4åˆ—ï¼šçœŸå® vs é¢„æµ‹æ•£ç‚¹å›¾
            ax4 = axes[row_idx, 3]
            ax4.scatter(residual_true, residual_pred, s=30, alpha=0.6, c='steelblue', edgecolors='k', linewidths=0.5)
            
            # æ·»åŠ  y=x å‚è€ƒçº¿
            lim_min = min(residual_true.min(), residual_pred.min())
            lim_max = max(residual_true.max(), residual_pred.max())
            ax4.plot([lim_min, lim_max], [lim_min, lim_max], 'r--', linewidth=2, label='Perfect fit', alpha=0.7)
            
            ax4.set_xlabel('Residual (True)', fontsize=10)
            ax4.set_ylabel('Residual (SVM pred)', fontsize=10)
            ax4.set_title(f'True vs Predicted\nRÂ²={residual_r2:.3f}, MAE={residual_mae:.3f}', fontsize=11, fontweight='bold')
            ax4.grid(True, alpha=0.3)
            ax4.legend(loc='best', fontsize=8)
            ax4.set_aspect('equal', adjustable='box')
        
        # æ€»æ ‡é¢˜
        mode_text = "Single Sufficiency" if self.single_sufficiency else "Multiple Sufficiency"
        svm_features = ', '.join(self.stage2_features)
        fig.suptitle(f'Stage 2 SVM Diagnosis | Mode: {mode_text} | Features: [{svm_features}]', 
                    fontsize=14, fontweight='bold', y=0.995)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡ï¼ˆå¦‚æœæŒ‡å®šäº†è·¯å¾„ï¼‰
        if save_path:
            import os
            os.makedirs(os.path.dirname(save_path), exist_ok=True) if os.path.dirname(save_path) else None
            file_format = os.path.splitext(save_path)[1][1:]  # è·å–æ‰©å±•åï¼ˆä¸å«ç‚¹ï¼‰
            plt.savefig(save_path, format=file_format, dpi=300, bbox_inches='tight')
            print(f"âœ“ Stage 2 diagram saved to: {save_path}")
        
        plt.show()
        
        # æ‰“å°è¯¦ç»†ç»Ÿè®¡
        print(f"\n{'='*70}")
        print(f"Stage 2 SVM Diagnosis Report")
        print(f"{'='*70}")
        print(f"Mode: {mode_text}")
        print(f"Training samples: {len(df)}")
        print(f"SVM features: {svm_features}")
        print(f"Resolution: {self.resolution}")
        print(f"")
        print(f"Overall Performance:")
        overall_r2 = r2_score(df['residual_true'], df['residual_pred'])
        overall_mae = mean_absolute_error(df['residual_true'], df['residual_pred'])
        print(f"  Residual prediction RÂ²: {overall_r2:.6f}")
        print(f"  Residual prediction MAE: {overall_mae:.6f}")
        print(f"  Residual range: [{df['residual_true'].min():.3f}, {df['residual_true'].max():.3f}]")
        print(f"  Original RÂ² range: [{df['r2'].min():.3f}, {df['r2'].max():.3f}]")
        print(f"{'='*70}\n")
    
    def predict(self, X_test):
        """
        é¢„æµ‹ï¼ˆå®Œæ•´åŒé˜¶æ®µï¼‰
        
        Args:
            X_test: æµ‹è¯•æ•°æ®ï¼Œå¿…é¡»åŒ…å« 4 åˆ—: [longitude, latitude, sufficiency_log, sparsity]
            
        Returns:
            y_pred: é¢„æµ‹å€¼ (GAM é¢„æµ‹ + SVM æ®‹å·®é¢„æµ‹)
        """
        if self.gam_model is None or self.svm_model is None:
            raise ValueError("Model not fitted yet. Call fit() first.")
        
        # å‡è®¾è¾“å…¥é¡ºåºï¼š[longitude, latitude, sufficiency_log, sparsity]
        feature_map = {
            'longitude': 0,
            'latitude': 1,
            'sufficiency_log': 2,
            'sparsity': 3
        }
        
        # Stage 2: SVM é¢„æµ‹æ®‹å·®ï¼ˆä½¿ç”¨å¯é…ç½®çš„ç‰¹å¾ï¼‰
        X_spatial = np.column_stack([X_test[:, feature_map[f]] for f in self.stage2_features])
        X_spatial_scaled = self.svm_scaler.transform(X_spatial)
        residual_pred = self.svm_model.predict(X_spatial_scaled)
        
        if self.svm_only:
            # SVM-only æ¨¡å¼ï¼šç›´æ¥è¿”å› SVM é¢„æµ‹
            return residual_pred
        else:
            # æ­£å¸¸æ¨¡å¼ï¼šStage 1 GAM é¢„æµ‹ï¼ˆæ ¹æ® sufficiency æ•°é‡é€‰æ‹©ç‰¹å¾ï¼‰
            if self.single_sufficiency:
                # åªç”¨ sparsity
                X_density = X_test[:, feature_map['sparsity']].reshape(-1, 1)
            else:
                # ç”¨ sufficiency_log å’Œ sparsity
                X_density = np.column_stack([
                    X_test[:, feature_map['sufficiency_log']],
                    X_test[:, feature_map['sparsity']]
                ])
            
            X_density_scaled = self.gam_scaler.transform(X_density)
            y_gam_pred = self.gam_model.predict(X_density_scaled)
            
            # æœ€ç»ˆé¢„æµ‹ = GAM é¢„æµ‹ + æ®‹å·®é¢„æµ‹
            y_pred = y_gam_pred + residual_pred
            
            return y_pred
    
    def predict_stage1_only(self, X_test):
        """
        ä»…ä½¿ç”¨ç¬¬ä¸€é˜¶æ®µ GAM (Density) é¢„æµ‹
        
        Args:
            X_test: æµ‹è¯•æ•°æ®ï¼Œæ”¯æŒä¸¤ç§æ ¼å¼ï¼š
                   - 2 åˆ—: [sufficiency_log, sparsity] ç›´æ¥æ˜¯densityç‰¹å¾
                   - 4 åˆ—: [longitude, latitude, sufficiency_log, sparsity] ä»ä¸­æå–densityç‰¹å¾
            
        Returns:
            y_pred: ä»… GAM çš„é¢„æµ‹å€¼ï¼ˆä¸åŒ…æ‹¬ SVM æ®‹å·®ï¼‰
        """
        if self.gam_model is None:
            raise ValueError("Model not fitted yet. Call fit() first.")
        
        n_cols = X_test.shape[1]
        
        # æ ¹æ®è¾“å…¥åˆ—æ•°å†³å®šå¦‚ä½•æå–ç‰¹å¾
        if n_cols == 2:
            # ç›´æ¥æ˜¯ [sufficiency_log, sparsity]
            if self.single_sufficiency:
                X_density = X_test[:, 1].reshape(-1, 1)  # åªç”¨ sparsity
            else:
                X_density = X_test  # ç”¨å…¨éƒ¨ä¸¤åˆ—
        elif n_cols == 4:
            # ä» [longitude, latitude, sufficiency_log, sparsity] ä¸­æå–
            if self.single_sufficiency:
                X_density = X_test[:, 3].reshape(-1, 1)  # åªç”¨ sparsity
            else:
                X_density = X_test[:, 2:4]  # æå– [sufficiency_log, sparsity]
        else:
            raise ValueError(f"X_test must have 2 or 4 columns, got {n_cols}")
        
        X_density_scaled = self.gam_scaler.transform(X_density)
        y_gam_pred = self.gam_model.predict(X_density_scaled)
        
        return y_gam_pred
    
    def predict_stage2_only(self, X_test):
        """
        ä»…ä½¿ç”¨ç¬¬äºŒé˜¶æ®µ SVM (Spatial) é¢„æµ‹æ®‹å·®
        
        Args:
            X_test: æµ‹è¯•æ•°æ®ï¼Œå¿…é¡»åŒ…å« 4 åˆ—: [longitude, latitude, sufficiency_log, sparsity]
            
        Returns:
            residual_pred: ä»… SVM çš„æ®‹å·®é¢„æµ‹å€¼
        """
        if self.svm_model is None:
            raise ValueError("Model not fitted yet. Call fit() first.")
        
        # å‡è®¾è¾“å…¥é¡ºåºï¼š[longitude, latitude, sufficiency_log, sparsity]
        feature_map = {
            'longitude': 0,
            'latitude': 1,
            'sufficiency_log': 2,
            'sparsity': 3
        }
        
        # Stage 2: SVM é¢„æµ‹æ®‹å·®
        X_spatial = np.column_stack([X_test[:, feature_map[f]] for f in self.stage2_features])
        X_spatial_scaled = self.svm_scaler.transform(X_spatial)
        residual_pred = self.svm_model.predict(X_spatial_scaled)
        
        return residual_pred


def _fit_single_model(model_type, X_train, X_test, y_train, y_test, metric='r2', spline=8, lam=1, return_model=False):
    """è®­ç»ƒå•ä¸ªæ¨¡å‹çš„è¾…åŠ©å‡½æ•°
    
    Args:
        model_type: æ¨¡å‹ç±»å‹ï¼Œæ”¯æŒ 'linear', 'ridge', 'elasticnet', 'rf', 'gbr', 
                   'lightgbm', 'svm', 'svm_with_constraint', 'gam_free', 'gam_monotonic', etc.
        metric: 'r2' or 'correlation'
        spline: GAMæ ·æ¡å‡½æ•°èŠ‚ç‚¹æ•°
        lam: GAMæ­£åˆ™åŒ–å‚æ•°
        return_model: å¦‚æœTrueï¼Œè¿”å›(score, model_obj, scaler)ï¼›å¦‚æœFalseï¼Œåªè¿”å›score
    """
    model_obj = None
    scaler = None
    
    try:
        if model_type == 'linear':
            # scaler = StandardScaler()
            # X_train = scaler.fit_transform(X_train)
            # X_test = scaler.transform(X_test) 

            model_obj = LinearRegression()
            model_obj.fit(X_train, y_train)
            y_pred = model_obj.predict(X_test)
            
        elif model_type == 'ridge':
            from sklearn.linear_model import Ridge
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            model_obj = Ridge(alpha=1.0, random_state=42)
            model_obj.fit(X_train_scaled, y_train)
            y_pred = model_obj.predict(X_test_scaled)
            
        elif model_type == 'elasticnet':
            from sklearn.linear_model import ElasticNet
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            model_obj = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)
            model_obj.fit(X_train_scaled, y_train)
            y_pred = model_obj.predict(X_test_scaled)
            
        elif model_type == 'rf':
            from sklearn.ensemble import RandomForestRegressor
            model_obj = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)
            model_obj.fit(X_train, y_train)
            y_pred = model_obj.predict(X_test)
            
        elif model_type == 'gbr':
            from sklearn.ensemble import GradientBoostingRegressor
            model_obj = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
            model_obj.fit(X_train, y_train)
            y_pred = model_obj.predict(X_test)
            
        elif model_type == 'lightgbm':
            if not HAS_LIGHTGBM:
                if return_model:
                    return -999, None, None
                return -999
            model_obj = LGBMRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, 
                                random_state=42, verbose=-1)
            model_obj.fit(X_train, y_train)
            y_pred = model_obj.predict(X_test)
            
        elif model_type == 'svm':
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            model_obj = SVR(kernel='rbf', C=1.0, gamma='scale')
            model_obj.fit(X_train_scaled, y_train)
            y_pred = model_obj.predict(X_test_scaled)
            
        elif model_type == 'svm_with_constraint':
            # SVM with light monotonic constraints on density and sufficiency
            # Strategy: Train SVM, then apply soft monotonic regularization
            # REQUIRES: 4 features [longitude, latitude, sufficiency_log, sparsity]
            # n_features = X_train.shape[1] 
            # if n_features != 4:
            #     raise ValueError(
            #         f"[longitude, latitude, sufficiency_log, sparsity], but got {n_features} features. "
            #     )
            
            # scaler = StandardScaler()
            # X_train_scaled = scaler.fit_transform(X_train)
            # X_test_scaled = scaler.transform(X_test)
            
            # # Train base SVM model
            # model_obj = SVR(kernel='rbf', C=1.0, gamma='scale')
            # model_obj.fit(X_train_scaled, y_train)
            # y_pred_raw = model_obj.predict(X_test_scaled)
            
            # Apply constraint: higher sufficiency/sparsity should have higher RÂ²
            # Use isotonic regression on these features as soft guidance
            from sklearn.isotonic import IsotonicRegression
            
            # Create a combined "density score" from sufficiency and sparsity
            # This is used to guide monotonic adjustment
            density_score_train = X_train[:, 2] + X_train[:, 3]  # sufficiency_log + sparsity
            density_score_test = X_test[:, 2] + X_test[:, 3]
            
            # Fit isotonic model as a guide (soft constraint)
            iso_reg = IsotonicRegression(increasing=True, out_of_bounds='clip')
            iso_reg.fit(density_score_train, y_train)
            y_iso = iso_reg.predict(density_score_test)
            
            # Blend SVM prediction with isotonic guidance (constraint weight adjustable)
            constraint_weight = 1  # High constraint weight
            y_pred = y_iso#(1 - constraint_weight) * y_pred_raw + constraint_weight * y_iso
            
        elif model_type == 'gam_free':
            from pygam import LinearGAM, s, te
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            spline=8
            lam=1
            # æ ¹æ®ç‰¹å¾æ•°é‡è‡ªé€‚åº”æ„å»ºGAM
            n_features = X_train.shape[1]
            if n_features == 2:
                model_obj = LinearGAM(
                    s(0, n_splines=spline, lam=lam) + 
                    s(1,  n_splines=spline, lam=lam) 
                    +te(0, 1, n_splines=[max(8, spline//4), max(8, spline//4)], lam=lam*2)
                )
            elif n_features == 4:
                model_obj = LinearGAM(
                    s(0, n_splines=spline, lam=lam) + 
                    s(1, n_splines=spline, lam=lam) + 
                    s(2, n_splines=spline, lam=lam) + 
                    s(3, n_splines=spline, lam=lam)
                                            +
                    te(0, 1, n_splines=[max(4, spline//4), max(4, spline//4)], lam=lam*2)
                    +
                    te(2, 3, n_splines=[max(4, spline//4), max(4, spline//4)], lam=lam*2)
                    +
                    te(0, 2, n_splines=[max(4, spline//4), max(4, spline//4)], lam=lam*2)
                    +
                    te(1, 2, n_splines=[max(4, spline//4), max(4, spline//4)], lam=lam*2)
                    +
                    te(0, 3, n_splines=[max(4, spline//4), max(4, spline//4)], lam=lam*2)
                    +
                    te(1, 3, n_splines=[max(4, spline//4), max(4, spline//4)], lam=lam*2)

                )
            else:
                # é€šç”¨æƒ…å†µï¼šæ ¹æ®ç‰¹å¾æ•°é‡åŠ¨æ€ç”Ÿæˆ
                terms = [s(i, n_splines=8, lam=1) for i in range(n_features)]
                model_obj = LinearGAM(sum(terms[1:], terms[0]))  # sum with initial value
            
            model_obj.fit(X_train_scaled, y_train)
            y_pred = model_obj.predict(X_test_scaled)
        
        elif model_type == 'gam_monotonic':
            # GAM with monotonic constraints
            from pygam import LinearGAM, s, te
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            n_features = X_train.shape[1]
            if n_features == 2:
                model_obj = LinearGAM(
                    s(0, constraints='monotonic_inc', n_splines=spline, lam=lam) + 
                    s(1, constraints='monotonic_inc', n_splines=spline, lam=lam) +
                    te(0, 1, n_splines=[max(4, spline//4), max(4, spline//4)], lam=lam*2)
                )
            elif n_features == 4:
                model_obj = LinearGAM(
                    s(0, n_splines=spline, lam=lam) +  # longitude (no constraint)
                    s(1, n_splines=spline, lam=lam) +  # latitude (no constraint)
                    s(2, constraints='monotonic_inc', n_splines=spline, lam=lam) +  # sufficiency_log
                    s(3, constraints='monotonic_inc', n_splines=spline, lam=lam) +  # sparsity
                    te(0, 1, n_splines=[max(4, spline//4), max(4, spline//4)], lam=lam*2) +
                    te(2, 3, n_splines=[max(4, spline//4), max(4, spline//4)], lam=lam*2) +
                    te(0, 2, n_splines=[max(4, spline//4), max(4, spline//4)], lam=lam*2) +
                    te(1, 2, n_splines=[max(4, spline//4), max(4, spline//4)], lam=lam*2) +
                    te(0, 3, n_splines=[max(4, spline//4), max(4, spline//4)], lam=lam*2) +
                    te(1, 3, n_splines=[max(4, spline//4), max(4, spline//4)], lam=lam*2)
                )
            else:
                # é€šç”¨æƒ…å†µï¼šæ‰€æœ‰ç‰¹å¾éƒ½åŠ monotonic
                terms = [s(i, constraints='monotonic_inc', n_splines=spline, lam=lam) for i in range(n_features)]
                model_obj = LinearGAM(sum(terms[1:], terms[0]))
            
            model_obj.fit(X_train_scaled, y_train)
            y_pred = model_obj.predict(X_test_scaled)
        
        elif model_type == 'gam_without_interaction':
            # GAM without interaction terms (only main effects with monotonic constraints)
            from pygam import LinearGAM, s
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            n_features = X_train.shape[1]
            if n_features == 2:
                model_obj = LinearGAM(
                    s(0, n_splines=spline, lam=lam) + 
                    s(1,  n_splines=spline, lam=lam)
                )
            elif n_features == 4:
                model_obj = LinearGAM(
                    s(0, n_splines=spline, lam=lam) +  # longitude (no constraint)
                    s(1, n_splines=spline, lam=lam) +  # latitude (no constraint)
                    s(2,  n_splines=spline, lam=lam) +  # sufficiency_log
                    s(3,  n_splines=spline, lam=lam)     # sparsity
                )
            else:
                # é€šç”¨æƒ…å†µ
                terms = [s(i,  n_splines=spline, lam=lam) for i in range(n_features)]
                model_obj = LinearGAM(sum(terms[1:], terms[0]))
            
            model_obj.fit(X_train_scaled, y_train)
            y_pred = model_obj.predict(X_test_scaled)
        
        elif model_type == 'gam_mono_noint':
            # GAM with Monotonic constraints + No Interaction (same as gam_without_interaction)
            from pygam import LinearGAM, s
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            n_features = X_train.shape[1]
            if n_features == 2:
                model_obj = LinearGAM(
                    s(0, constraints='monotonic_inc', n_splines=spline, lam=lam) + 
                    s(1, constraints='monotonic_inc', n_splines=spline, lam=lam)
                )
            elif n_features == 4:
                model_obj = LinearGAM(
                    s(0, n_splines=spline, lam=lam) +  # longitude (no constraint)
                    s(1, n_splines=spline, lam=lam) +  # latitude (no constraint)
                    s(2, constraints='monotonic_inc', n_splines=spline, lam=lam) +  # sufficiency_log
                    s(3, constraints='monotonic_inc', n_splines=spline, lam=lam)     # sparsity
                )
            else:
                # é€šç”¨æƒ…å†µ
                terms = [s(i, constraints='monotonic_inc', n_splines=spline, lam=lam) for i in range(n_features)]
                model_obj = LinearGAM(sum(terms[1:], terms[0]))
            
            model_obj.fit(X_train_scaled, y_train)
            y_pred = model_obj.predict(X_test_scaled)
            
        elif model_type == 'interpolation':
            # IDW (Inverse Distance Weighting) ç©ºé—´æ’å€¼
            # åˆ›å»º IDW æ’å€¼å™¨ï¼Œæä¾›ç»Ÿä¸€çš„ .predict() æ¥å£
            model_obj = IDWInterpolator(X_train, y_train)
            y_pred = model_obj.predict(X_test)
        
        else:
            if return_model:
                return -999, None, None
            return -999
        
        # æ ¹æ®metricé€‰æ‹©è¯„ä¼°æ–¹å¼
        if metric == 'correlation':
            from scipy.stats import pearsonr
            corr, _ = pearsonr(y_test, y_pred)
            score = corr
        else:  # r2
            score = r2_score(y_test, y_pred)
        
        if return_model:
            return score, model_obj, scaler
        else:
            return score
            
    except Exception as e:
        # æŠ›å‡ºè¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ï¼Œè€Œä¸æ˜¯è¿”å› -999
        raise RuntimeError(f"Failed to fit {model_type} model: {str(e)}") from e

def eval_baseline_comparison(df, sparsity_bins, split_by='grid', 
                             lam=5.0, spline=14, use_seeds=True, metric='r2', full_features='Density', 
                             resolution=None, split_method='spatial', train_by='grid', evaluate_by='grid',spline_order=3,
                             model_list=None, clip=None):
    """
    åŸºäºç»Ÿä¸€æ•°æ®åˆ†å‰²çš„åŸºçº¿æ¨¡å‹å¯¹æ¯”è¯„ä¼°
    
    æ‰€æœ‰åŸºçº¿æ–¹æ³•éƒ½åŸºäºç»Ÿä¸€çš„ç©ºé—´æ•°æ®åˆ†å‰²è¿›è¡Œå…¬å¹³æ¯”è¾ƒ
    
    Args:
        df: åŸå§‹æ•°æ®æ¡†
        sparsity_bins: sparsityåˆ†binæ•°é‡
        split_by: 'grid' or 'station' - ç©ºé—´åˆ†å‰²æ–¹å¼
        lam: GAMæ­£åˆ™åŒ–å‚æ•°
        spline: GAMæ ·æ¡å‡½æ•°èŠ‚ç‚¹æ•°
        use_seeds: æ˜¯å¦ä½¿ç”¨seedsèšåˆ
        metric: 'r2' or 'correlation' - è¯„ä¼°æŒ‡æ ‡é€‰æ‹©
        full_features: 'Density', 'Spatial', or 'Full'
                      'Density': ä½¿ç”¨[sufficiency_log, sparsity]
                      'Spatial': ä½¿ç”¨[longitude, latitude]
                      'Full': ä½¿ç”¨å…¨éƒ¨4ä¸ªç‰¹å¾[longitude, latitude, sufficiency_log, sparsity]
        resolution: [lon_bins, lat_bins], é»˜è®¤[10, 10] - æ§åˆ¶spatial gridçš„åˆ†è¾¨ç‡
        split_method: è®­ç»ƒ/æµ‹è¯•æ•°æ®åˆ†å‰²æ–¹å¼
                     'spatial': æŒ‰ split_by è¿›è¡Œç©ºé—´åˆ†å‰²ï¼ˆé»˜è®¤ï¼‰
                     'sampling': æŒ‰ sufficiencyÃ—sparsity bins åˆ†å‰²
        train_by: è®­ç»ƒæ•°æ®èšåˆæ–¹å¼
                 - 'grid': æŒ‰ç½‘æ ¼èšåˆè®­ç»ƒæ•°æ®ï¼ˆé»˜è®¤ï¼‰
                 - 'station': æŒ‰ç«™ç‚¹èšåˆè®­ç»ƒæ•°æ®
                 - 'sampling': æŒ‰ sufficiencyÃ—sparsity bins èšåˆè®­ç»ƒæ•°æ®
        evaluate_by: æµ‹è¯•æ•°æ®èšåˆæ–¹å¼ï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨
                    - å­—ç¬¦ä¸²: 'grid', 'station', æˆ– 'sampling'
                    - åˆ—è¡¨: ['grid', 'station', 'sampling'] - å°†å¯¹æ¯ç§æ–¹å¼åˆ†åˆ«è¯„ä¼°å¹¶è¾“å‡ºè¡¨æ ¼
        model_list: è¦è®­ç»ƒçš„æ¨¡å‹åˆ—è¡¨ã€‚å¦‚æœä¸º Noneï¼Œåˆ™è®­ç»ƒæ‰€æœ‰åŸºçº¿æ¨¡å‹
                   é»˜è®¤: ['linear', 'ridge', 'elasticnet', 'rf', 'gbr', 'lightgbm', 'svm',
                          'gam_free', 'gam_monotonic', 'gam_without_interaction', 'gam_mono_noint', 
                          'interpolation', 'two_stage', 'two_stage_1']
                   å…¶ä¸­åŒé˜¶æ®µæ¨¡å‹åŒ…æ‹¬ï¼š
                   - 'two_stage': å®Œæ•´åŒé˜¶æ®µæ¨¡å‹
                     * Stage 1: GAM_Monotonic (åŸºäº density ç‰¹å¾é¢„æµ‹åŸºç¡€ RÂ²)
                     * Stage 2: SVM (åŸºäº spatial ç‰¹å¾é¢„æµ‹æ®‹å·®)
                     * Final: é¢„æµ‹ = GAM é¢„æµ‹ + SVM æ®‹å·®é¢„æµ‹
                   - 'two_stage_1': ä»…ä½¿ç”¨ç¬¬ä¸€é˜¶æ®µ
                     * Stage 1: GAM_Monotonic (åŸºäº density ç‰¹å¾é¢„æµ‹åŸºç¡€ RÂ²)
                     * Final: é¢„æµ‹ = GAM é¢„æµ‹ï¼ˆä¸åŒ…æ‹¬ SVM æ®‹å·®ï¼‰
                   - 'two_stage_2': ä»…ä½¿ç”¨ç¬¬äºŒé˜¶æ®µ
                     * Stage 2: SVM (åŸºäº spatial ç‰¹å¾é¢„æµ‹æ®‹å·®)
                     * Final: é¢„æµ‹ = SVM æ®‹å·®é¢„æµ‹ï¼ˆç›¸å¯¹äº GAM åŸºçº¿çš„åå·®ï¼‰
        
    Returns:
        dict: å¦‚æœ evaluate_by æ˜¯å­—ç¬¦ä¸²ï¼Œè¿”å›å•ä¸ªç»“æœå­—å…¸
              å¦‚æœ evaluate_by æ˜¯åˆ—è¡¨ï¼Œè¿”å› {evaluate_method: results_dict} çš„åµŒå¥—å­—å…¸
    """
    if resolution is None:
        resolution = [10, 10]
    
    # è®¾ç½®é»˜è®¤æ¨¡å‹åˆ—è¡¨
    if model_list is None:
        model_list = [
            'linear', 'ridge', 'elasticnet', 'rf', 'gbr', 'lightgbm', 'svm',
            'gam_free', 'gam_monotonic', 'gam_without_interaction', 'gam_mono_noint', 
            'interpolation', 'two_stage', 'two_stage_1', 'two_stage_2'
        ]
    
    # è½¬æ¢ evaluate_by ä¸ºåˆ—è¡¨
    if isinstance(evaluate_by, str):
        evaluate_by_list = [evaluate_by]
        return_single = True
    else:
        evaluate_by_list = list(evaluate_by)
        return_single = False
    
    # æ£€æŸ¥æ•°æ®
    model_columns = [col for col in df.columns if col.startswith('predicted_')]
    model_names = [col.replace('predicted_', '') for col in model_columns]
    
    # ========================================
    # Step 1: Split raw data and prepare bin intervals
    # ========================================
    df_copy = df.copy()
    
    # å…ˆè®¡ç®—å…¨å±€ bin intervalsï¼ˆç”¨äºåç»­èšåˆï¼‰
    print(f"\n=== Step 1: Calculate bin intervals and split data ===")
    bins_intervals = find_bins_intervals(df_copy, sparsity_bins)
    
    print(f"  Split method: {split_method}")
    if split_method == 'spatial':
        # Spatial split by grid or station
        df_train_raw, df_test_raw = spatial_split(df_copy, split_by=split_by, train_ratio=0.7, seed=42)
        
    elif split_method == 'sampling':
        # Sampling-based split (by sufficiencyÃ—sparsity bins)
        df_train_raw, df_test_raw, train_ids, test_ids, _ = sampling_split_raw(
            df_copy, 
            sparsity_bins=sparsity_bins, 
            train_ratio=0.7, 
            seed=42,
            use_seeds=use_seeds
        )
    print(f"  Train samples: {len(df_train_raw)}, Test samples: {len(df_test_raw)}")
    
    # ========================================
    # Step 2: èšåˆè®­ç»ƒæ•°æ®ï¼ˆç»Ÿä¸€ç”Ÿæˆæ‰€æœ‰ç‰¹å¾ï¼‰
    # ========================================
    print(f"\n=== Step 2: Aggregate train data ===")
    
    # ä¸º Interpolation å‡†å¤‡çº¯ spatial è®­ç»ƒæ•°æ®ï¼ˆæ‰€æœ‰æ¨¡å¼å…±ç”¨ï¼Œä¸å— train_by å½±å“ï¼‰
    print(f"  Preparing pure spatial data for interpolation baseline (split_by={split_by})...")
    spatial_train_dict_pure = prepare_spatial_features(
        df_train_raw, model_names,
        split_by=split_by,
        include_sampling_features=False,  # ä¸åŒ…å« sampling features
        bins_intervals=None,
        resolution=resolution,
        clip=clip  # ä¼ é€’æˆªæ–­å‚æ•°
    )
    
    # æ ¹æ® train_by é€‰æ‹©èšåˆæ–¹å¼ï¼ˆç”¨äºå…¶ä»–æ¨¡å‹ï¼‰
    if train_by == 'sampling':
        print(f"  Aggregating train data by sufficiencyÃ—sparsity bins...")
        train_data_dict = prepare_sampling_features(
            bins_intervals, df_train_raw, model_names, use_seeds,
            include_spatial_coords=True,
            clip=clip  # ä¼ é€’æˆªæ–­å‚æ•°
        )
    else:
        # train_by = 'grid' or 'station'
        train_split_by = 'station' if train_by == 'station' else 'grid'
        print(f"  Aggregating train data by spatial {train_split_by} with resolution {resolution}...")
        train_data_dict = prepare_spatial_features(
            df_train_raw, model_names, 
            split_by=train_split_by, 
            include_sampling_features=True,  # å§‹ç»ˆç”Ÿæˆæ‰€æœ‰ç‰¹å¾
            bins_intervals=bins_intervals,
            resolution=resolution,
            clip=clip  # ä¼ é€’æˆªæ–­å‚æ•°
        )
    
    # ========================================
    # Step 3: è®­ç»ƒæ¨¡å‹ï¼ˆåªè®­ç»ƒä¸€æ¬¡ï¼‰
    # ========================================
    print(f"\n=== Step 3: Training models (train_by={train_by}, features={full_features}) ===")
    
    # ç‰¹å¾é€‰æ‹©é…ç½®
    feature_configs = {
        'Full': ['longitude', 'latitude', 'sufficiency_log', 'sparsity'],
        'Spatial': ['longitude', 'latitude'],
        'Density': ['sufficiency_log', 'sparsity']
    }
    feature_cols = feature_configs[full_features]
    
    # å­˜å‚¨è®­ç»ƒå¥½çš„æ¨¡å‹
    trained_models = {}
    
    for model_name in tqdm(model_names, desc="Training models"):
        model_train_data = train_data_dict.get(model_name, pd.DataFrame())
        spatial_train_data = spatial_train_dict_pure.get(model_name, pd.DataFrame())
        
        if len(model_train_data) < 5:
            trained_models[model_name] = None
            continue
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        X_train = model_train_data[feature_cols].values
        y_train = model_train_data['r2'].values
        
        # è®­ç»ƒæ‰€æœ‰æ¨¡å‹ï¼ˆåªè®­ç»ƒä¸€æ¬¡ï¼ï¼‰
        model_dict = {}
        for model_type in model_list:
            if model_type == 'interpolation':
                # Interpolation: ä½¿ç”¨çº¯ spatial æ•°æ®å°è£…è®­ç»ƒæ•°æ®
                X_train_spatial = spatial_train_data[['longitude', 'latitude']].values
                y_train_spatial = spatial_train_data['r2'].values
                _, trained_model, scaler = _fit_single_model(
                    'interpolation', X_train_spatial, X_train_spatial, y_train_spatial, y_train_spatial,
                    metric=metric, spline=spline, lam=lam, return_model=True
                )
                model_dict['interpolation'] = {'model': trained_model, 'scaler': scaler, 'type': 'interpolation'}
            elif model_type in ['two_stage', 'two_stage_1', 'two_stage_2']:
                # Two-Stage Model: GAM (Density) + SVM (Spatial Residual)
                # two_stage_1: Only use Stage 1 (GAM) prediction
                # two_stage_2: Only use Stage 2 (SVM) residual prediction
                try:
                    two_stage_model = TwoStageModel(spline=spline, lam=lam, resolution=resolution, diagnose=False, spline_order=spline_order)
                    stage1_score, stage2_score = two_stage_model.fit(
                        df_train_raw, model_name, bins_intervals, 
                        use_seeds=use_seeds, split_by=split_by, metric=metric
                    )
                    model_dict[model_type] = {'model': two_stage_model, 'scaler': None, 'type': model_type}
                except ValueError as e:
                    # ValueError é€šå¸¸æ˜¯è‡´å‘½é”™è¯¯ï¼ˆå¦‚æ•°æ®ä¸è¶³ï¼‰ï¼Œåº”è¯¥æŠ›å‡º
                    error_msg = str(e)
                    if "Insufficient training samples" in error_msg:
                        raise  # æ•°æ®ä¸è¶³é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                    else:
                        print(f"    Warning: {model_type} training failed for {model_name}: {error_msg}")
                except Exception as e:
                    print(f"    Warning: {model_type} training failed for {model_name}: {e}")
                    model_dict[model_type] = None
            else:
                # æ™®é€šæ¨¡å‹ï¼šè®­ç»ƒ
                _, trained_model, scaler = _fit_single_model(
                    model_type, X_train, X_train, y_train, y_train,
                    metric=metric, spline=spline, lam=lam, return_model=True
                )
                model_dict[model_type] = {'model': trained_model, 'scaler': scaler, 'type': model_type}
        
        trained_models[model_name] = model_dict
    
    # ========================================
    # Step 4: åœ¨ä¸åŒæµ‹è¯•æ•°æ®ä¸Šè¯„ä¼°ï¼ˆè¯„ä¼°å¤šæ¬¡ï¼‰
    # ========================================
    all_results = {}
    
    for eval_method in evaluate_by_list:
        print(f"\n{'='*80}")
        print(f"=== Evaluating on: {eval_method.upper()} ===")
        print(f"{'='*80}")
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        if eval_method == 'sampling':
            print(f"  Aggregating test data by sufficiencyÃ—sparsity bins...")
            test_data_dict = prepare_sampling_features(
                bins_intervals, df_test_raw, model_names, use_seeds,
                include_spatial_coords=True,
                clip=clip  # ä¼ é€’æˆªæ–­å‚æ•°
            )
        else:
            test_split_by = 'station' if eval_method == 'station' else 'grid'
            print(f"  Aggregating test data by spatial {test_split_by} with resolution {resolution}...")
            test_data_dict = prepare_spatial_features(
                df_test_raw, model_names, 
                split_by=test_split_by, 
                include_sampling_features=True,
                bins_intervals=bins_intervals,
                resolution=resolution,
                clip=clip  # ä¼ é€’æˆªæ–­å‚æ•°
            )
        
        # ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œè¯„ä¼°
        results = _evaluate_trained_models(
            trained_models, test_data_dict, model_names, 
            model_list, feature_cols, metric
        )
        
        # æ‰“å°å½“å‰è¯„ä¼°æ–¹å¼çš„ç»“æœ
        _print_performance_table(results, model_list=model_list, metric=metric)
        all_results[eval_method] = results
    
    # è¿”å›ç»“æœ
    if return_single:
        return all_results[evaluate_by_list[0]]
    else:
        return all_results


def analyze_model_performance(df, sparsity_bins=None, use_seeds=True, include_spatial=True, 
                             lam=None, spline=None, target_model='Ours', metric='r2'):
    """
    å®Œæ•´çš„æ¨¡å‹æ€§èƒ½åˆ†æ - Original æ¨¡å¼ï¼ˆå¤‡ä»½å‡½æ•°ï¼‰
    
    ä½¿ç”¨åŸå§‹æ–¹æ³•ï¼šsufficiency/sparsity å„è‡ªrandom split, spatial å„è‡ªrandom split
    æ­¤å‡½æ•°ä¿ç•™ä½œä¸ºå¤‡ä»½ï¼Œç”¨äºä¸æ–°æ–¹æ³•å¯¹æ¯”
    
    Args:
        df: åŸå§‹æ•°æ®æ¡†
        sparsity_bins: sparsityåˆ†binæ•°é‡ï¼Œå¦‚æœNoneåˆ™è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜
        use_seeds: æ˜¯å¦ä½¿ç”¨seedsèšåˆ
        include_spatial: æ˜¯å¦åŒ…å«ç©ºé—´åˆ†æ
        lam: GAMæ­£åˆ™åŒ–å‚æ•°ï¼Œå¦‚æœNoneåˆ™è‡ªåŠ¨æœç´¢æœ€ä¼˜å€¼
        spline: GAMæ ·æ¡å‡½æ•°èŠ‚ç‚¹æ•°ï¼Œå¦‚æœNoneåˆ™è‡ªåŠ¨æœç´¢æœ€ä¼˜å€¼
        target_model: ç”¨äºGAMå‚æ•°ä¼˜åŒ–çš„ç›®æ ‡æ¨¡å‹
        metric: 'r2' or 'correlation' - è¯„ä¼°æŒ‡æ ‡é€‰æ‹©
        
    Returns:
        dict: åŒ…å«æ‰€æœ‰æ¨¡å‹æ€§èƒ½ç»“æœçš„å­—å…¸
    """
    resolution = [10, 10]  # Fixed resolution for original mode
    
    print("=== Ozone Reconstruction Model Performance Analysis (Original Mode - Backup) ===")
    
    # æ£€æŸ¥æ•°æ®
    model_columns = [col for col in df.columns if col.startswith('predicted_')]
    has_spatial = include_spatial and all(col in df.columns for col in ['longitude', 'latitude'])
    
    print(f"Available models: {[col.replace('predicted_', '') for col in model_columns]}")
    print(f"Spatial analysis: {'Enabled' if has_spatial else 'Disabled'}")
    
    # 1. ç¡®å®šæœ€ä¼˜sparsity_bins
    if sparsity_bins is None:
        print("Finding optimal sparsity bins...")
        sparsity_bins = find_optimal_sparsity_bins(df, use_seeds=use_seeds, target_model=target_model)
    
    # 2. ç¡®å®šGAMè¶…å‚æ•°
    final_lam = lam
    final_spline = spline
    
    if lam is None or spline is None:
        print("Optimizing GAM hyperparameters...")
        try:
            optimal_lam, optimal_spline, gam_score = find_optimal_gam_params(
                df, sparsity_bins, target_model=target_model, use_seeds=use_seeds
            )
            
            if lam is None:
                final_lam = optimal_lam
            if spline is None:
                final_spline = optimal_spline
                
            print(f"Optimal GAM parameters: lam={final_lam:.3f}, spline={final_spline}")
            
        except Exception as e:
            print(f"GAM optimization failed: {e}")
            print("Using default GAM parameters...")
            final_lam = final_lam or 5.0
            final_spline = final_spline or 14
    else:
        print(f"Using specified GAM parameters: lam={final_lam:.3f}, spline={final_spline}")
    
    # 3. è®¡ç®—RÂ²æ•°æ®
    print("Calculating RÂ² data with bins...")
    df_bins = calculate_r2_with_bins(df, sparsity_bins, use_seeds=use_seeds, verbose=False)
    
    if len(df_bins) == 0:
        print("No data generated for model comparison!")
        return {}
    
    # 4. è®­ç»ƒGAMæ¨¡å‹
    print("Fitting GAM models with optimized parameters...")
    gam_results = fit_gam_models(df, sparsity_bins, use_seeds=use_seeds, 
                                spline=final_spline, lam=final_lam, verbose=False)
    
    # 5. è®­ç»ƒå…¶ä»–æ¨¡å‹
    print("Training comparison models...")
    results = {}
    
    for model_name in tqdm(df_bins['model'].unique(), desc="Processing models"):
        model_data = df_bins[df_bins['model'] == model_name].copy()
        
        if len(model_data) < 10:
            continue
        
        # å‡†å¤‡æ•°æ®
        model_data['sufficiency_log'] = np.log10(model_data['sufficiency'])
        X = model_data[['sufficiency_log', 'sparsity']].values
        y = model_data['r2'].values
        
        # æ•°æ®åˆ†å‰²ï¼ˆåŸå§‹æ–¹æ³•ï¼šéšæœºsplitï¼‰
        if len(X) < 15:
            X_train = X_test = X
            y_train = y_test = y
        else:
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
        
        # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
        model_results = {}
        for model_type in ['linear', 'ridge', 'elasticnet', 'rf', 'gbr', 'lightgbm', 'svm', 'gam_free']:
            model_results[model_type] = _fit_single_model(model_type, X_train, X_test, y_train, y_test)
        
        # æ·»åŠ GAMç»“æœ
        model_results['gam_monotonic'] = gam_results.get(model_name, {}).get('r2_score', -999)
        
        # ç©ºé—´åˆ†æï¼ˆåŸå§‹æ–¹æ³•ï¼šç‹¬ç«‹random splitï¼‰
        if has_spatial:
            spatial_data = _calculate_spatial_r2(df, model_name, resolution=resolution)
            if len(spatial_data) > 0:
                X_spatial = spatial_data[['longitude', 'latitude']].values
                y_spatial = spatial_data['r2'].values
                
                if len(X_spatial) >= 15:
                    X_train_sp, X_test_sp, y_train_sp, y_test_sp = train_test_split(
                        X_spatial, y_spatial, test_size=0.3, random_state=42)
                else:
                    X_train_sp = X_test_sp = X_spatial
                    y_train_sp = y_test_sp = y_spatial
                
                model_results['interpolation'] = _fit_single_model('interpolation', X_train_sp, X_test_sp, y_train_sp, y_test_sp)
            else:
                model_results['interpolation'] = -999
        
        results[model_name] = model_results
    
    # 6. æ‰“å°ç»“æœ
    default_model_list = [
        'linear', 'ridge', 'elasticnet', 'rf', 'gbr', 'lightgbm', 'svm',
        'gam_free', 'gam_monotonic', 'gam_without_interaction', 'gam_mono_noint'
    ]
    if has_spatial:
        default_model_list.append('interpolation')
    _print_performance_table(results, model_list=default_model_list, metric=metric)
    
    # 7. æ‰“å°ä½¿ç”¨çš„å‚æ•°
    print(f"\nUsed parameters:")
    print(f"  Sparsity bins: {sparsity_bins}")
    print(f"  GAM lam: {final_lam:.3f}")
    print(f"  GAM spline: {final_spline}")
    print(f"  Target model for optimization: {target_model}")
    
    return results

def sampling_split_raw(df, sparsity_bins, train_ratio=0.85, seed=42, use_seeds=True):
    """
    æŒ‰sampling bins (sufficiencyÃ—sparsityç»„åˆ)åˆ†å‰²åŸå§‹æ•°æ®
    
    Args:
        df: åŸå§‹æ•°æ®
        sparsity_bins: sparsityåˆ†binæ•°é‡
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        seed: éšæœºç§å­
        use_seeds: æ˜¯å¦ä½¿ç”¨seedsèšåˆ
    
    Returns:
        df_train, df_test, train_ids, test_ids, bins_intervals
    """
    # Step 1: è®¡ç®—bin intervals
    bins_intervals = find_bins_intervals(df, sparsity_bins)
    sparsity_bins_edges, suff_to_bin = bins_intervals
    
    # Step 2: ç»™åŸå§‹dfåˆ†é…bins
    df_copy = df.copy()
    df_copy['sparsity_bin'] = pd.cut(df_copy['sparsity'], bins=sparsity_bins_edges, labels=False)
    df_copy['sufficiency_bin'] = df_copy['sufficiency'].map(suff_to_bin)
    df_copy = df_copy.dropna(subset=['sparsity_bin', 'sufficiency_bin'])
    
    # Step 3: åˆ›å»ºpseudo IDå¹¶splitï¼ˆä½¿ç”¨bin IDè€Œä¸æ˜¯åŸå§‹å€¼ï¼ï¼‰
    df_copy['pseudo_id'] = 'suf_' + df_copy['sufficiency_bin'].astype(str) + '_spa_' + df_copy['sparsity_bin'].astype(str)
    
    unique_ids = df_copy['pseudo_id'].unique()
    np.random.seed(seed)
    train_ids = np.random.choice(unique_ids, size=int(len(unique_ids) * train_ratio), replace=False)
    test_ids = np.array([pid for pid in unique_ids if pid not in train_ids])
    
    df_train = df_copy[df_copy['pseudo_id'].isin(train_ids)].copy()
    df_test = df_copy[df_copy['pseudo_id'].isin(test_ids)].copy()
    
    return df_train, df_test, train_ids, test_ids, bins_intervals


def sampling_split(df_bins_all, train_ratio=0.85, seed=42):
    """
    [DEPRECATED] æŒ‰sampling bins (sufficiencyÃ—sparsityç»„åˆ)åˆ†å‰²æ•°æ®
    
    æ­¤å‡½æ•°å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨ sampling_split_raw() ä»£æ›¿ã€‚
    è¯¥å‡½æ•°ç”¨äºæ—§æµç¨‹ï¼ˆå…ˆèšåˆå…¨éƒ¨æ•°æ®ï¼Œå†åˆ†å‰²ï¼‰ï¼Œä¸æ¨èä½¿ç”¨ã€‚
    
    Args:
        df_bins_all: å·²ç»æŒ‰binsèšåˆçš„æ•°æ®ï¼ˆæ¥è‡ªcalculate_r2_with_binsï¼‰
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        seed: éšæœºç§å­
    
    Returns:
        df_bins_train, df_bins_test, train_ids, test_ids
    """
    import warnings
    warnings.warn(
        "sampling_split() is deprecated. Use sampling_split_raw() instead for splitting raw data before aggregation.",
        DeprecationWarning,
        stacklevel=2
    )
    
    # åˆ›å»ºpseudo IDï¼ˆæ¯ä¸ªsufficiency/sparsityç»„åˆï¼‰
    df_bins_all['pseudo_id'] = 'suf_' + df_bins_all['sufficiency'].astype(str) + '_spa_' + df_bins_all['sparsity'].astype(str)
    
    unique_ids = df_bins_all['pseudo_id'].unique()
    
    # æŒ‰pseudo ID split
    np.random.seed(seed)
    train_ids = np.random.choice(unique_ids, size=int(len(unique_ids) * train_ratio), replace=False)
    test_ids = np.array([pid for pid in unique_ids if pid not in train_ids])
    
    df_bins_train = df_bins_all[df_bins_all['pseudo_id'].isin(train_ids)].copy()
    df_bins_test = df_bins_all[df_bins_all['pseudo_id'].isin(test_ids)].copy()
    
    return df_bins_train, df_bins_test, train_ids, test_ids


def spatial_split(df, split_by='grid', train_ratio=0.85, seed=42):
    """
    æŒ‰spatialæ–¹å¼åˆ†å‰²æ•°æ®
    
    Args:
        df: åŸå§‹æ•°æ®
        split_by: 'grid' or 'station'
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        seed: éšæœºç§å­
    
    Returns:
        df_train, df_test
    """
    df_copy = df.copy()
    
    if split_by == 'grid':
        # æŒ‰10x10ç©ºé—´ç½‘æ ¼åˆ†å‰²
        df_copy['longitude_bin'] = pd.cut(df_copy['longitude'], bins=10, labels=False)
        df_copy['latitude_bin'] = pd.cut(df_copy['latitude'], bins=10, labels=False)
        df_copy['split_id'] = 'grid_' + df_copy['longitude_bin'].astype(str) + '_' + df_copy['latitude_bin'].astype(str)
    else:  # station
        if 'Site_number' in df_copy.columns:
            df_copy['split_id'] = df_copy['Site_number']
        else:
            # ç”¨ç»çº¬åº¦ç»„åˆä½œä¸ºç«™ç‚¹ID
            df_copy['split_id'] = df_copy.groupby(['longitude', 'latitude']).ngroup()
    
    df_copy = df_copy.dropna(subset=['split_id'])
    
    unique_ids = df_copy['split_id'].unique()
    np.random.seed(seed)
    train_ids = np.random.choice(unique_ids, size=int(len(unique_ids) * train_ratio), replace=False)
    test_ids = np.array([sid for sid in unique_ids if sid not in train_ids])
    
    df_train = df_copy[df_copy['split_id'].isin(train_ids)].copy()
    df_test = df_copy[df_copy['split_id'].isin(test_ids)].copy()
    
    return df_train, df_test


def find_bins_intervals(df, sparsity_bins):
    """
    åœ¨å…¨å±€dfä¸Šæ‰¾åˆ°binè¾¹ç•Œ
    
    Returns:
        sparsity_bins_edges, sufficiency_to_bin_mapping
    """
    # Sparsity binè¾¹ç•Œ
    sparsity_cut_result = pd.cut(df['sparsity'], bins=sparsity_bins, labels=False, retbins=True)
    sparsity_bins_edges = sparsity_cut_result[1]
    
    # Sufficiency binæ˜ å°„ï¼ˆæ¨¡ä»¿calculate_r2_with_binsï¼‰
    unique_suff_values = np.sort(df['sufficiency'].unique())
    n_unique = len(unique_suff_values)
    sufficiency_bins_num = n_unique
    values_per_bin = n_unique // sufficiency_bins_num
    remainder = n_unique % sufficiency_bins_num
    
    suff_to_bin = {}
    current_idx = 0
    for bin_idx in range(sufficiency_bins_num):
        current_bin_size = values_per_bin + (1 if bin_idx < remainder else 0)
        for i in range(current_bin_size):
            if current_idx < n_unique:
                suff_to_bin[unique_suff_values[current_idx]] = bin_idx
                current_idx += 1
    
    return sparsity_bins_edges, suff_to_bin


def prepare_sampling_features(bins_intervals, df, model_names, use_seeds=False, include_spatial_coords=False, clip=None):
    """
    å‡†å¤‡è®­ç»ƒæ•°æ®ï¼šæŒ‰sufficiency/sparsity binsèšåˆ
    
    Args:
        bins_intervals: (sparsity_bins_edges, suff_to_bin)
        df: è®­ç»ƒæ•°æ®
        model_names: æ¨¡å‹åç§°åˆ—è¡¨
        include_spatial_coords: å¦‚æœTrueï¼Œä¹ŸåŒ…å«longitude/latitudeå‡å€¼
        clip: RÂ² æˆªæ–­èŒƒå›´ [min, max]ï¼Œåœ¨è®¡ç®— RÂ² åç«‹å³æˆªæ–­
              ä¾‹å¦‚ [-0.5, 1.0] è¡¨ç¤ºå°† RÂ² é™åˆ¶åœ¨ [-0.5, 1.0] èŒƒå›´å†…
              None è¡¨ç¤ºä¸æˆªæ–­
    
    Returns:
        {model_name: DataFrame with columns [sufficiency_log, sparsity, r2] 
                     or [sufficiency_log, sparsity, longitude, latitude, r2] if include_spatial_coords=True}
    """
    sparsity_bins_edges, suff_to_bin = bins_intervals
    
    df_copy = df.copy()
    df_copy['sparsity_bin'] = pd.cut(df_copy['sparsity'], bins=sparsity_bins_edges, labels=False)
    df_copy['sufficiency_bin'] = df_copy['sufficiency'].map(suff_to_bin)
    df_copy = df_copy.dropna(subset=['sparsity_bin', 'sufficiency_bin'])
    
    results_dict = {}
    
    for model_name in model_names:
        model_col = f'predicted_{model_name}'
        if model_col not in df_copy.columns:
            continue
        
        results = []
        group_cols = ['sufficiency_bin', 'sparsity_bin']
        
        for name, group in df_copy.groupby(group_cols):
            if len(group) < 10:
                continue
            
            observed = group['observed'].values
            predicted = group[model_col].values
            valid_mask = ~(np.isnan(observed) | np.isnan(predicted) | 
                          np.isinf(observed) | np.isinf(predicted))
            
            if valid_mask.sum() < 5:
                continue
            
            from sklearn.metrics import r2_score
            r2 = r2_score(observed[valid_mask], predicted[valid_mask])
            
            # ğŸ”§ Clip RÂ² å€¼ï¼ˆåœ¨æºæ•°æ®é˜¶æ®µï¼‰
            if clip is not None and len(clip) == 2:
                r2 = np.clip(r2, clip[0], clip[1])
            
            result = {
                'sufficiency_log': np.log10(group['sufficiency'].mean()),
                'sparsity': group['sparsity'].mean(),
                'sufficiency_bin': name[0],  # ä¿ç•™ bin ä¿¡æ¯
                'sparsity_bin': name[1],     # ä¿ç•™ bin ä¿¡æ¯
                'r2': r2
            }
            
            if include_spatial_coords:
                result['longitude'] = group['longitude'].mean()
                result['latitude'] = group['latitude'].mean()
            
            results.append(result)
        
        results_dict[model_name] = pd.DataFrame(results)
    
    return results_dict


def prepare_sampling_features_space(bins_intervals, df, model_names, split_by='grid', resolution=None, use_seeds=False, clip=None):
    """
    ä¸¤é˜¶æ®µèšåˆï¼šå…ˆæŒ‰ç©ºé—´ç½‘æ ¼èšåˆï¼Œå†æŒ‰sufficiency/sparsity binsèšåˆ
    
    è¿™æ˜¯ prepare_sampling_features çš„ç©ºé—´å¢å¼ºç‰ˆæœ¬ï¼š
    - prepare_sampling_features: ç›´æ¥å¯¹åŸå§‹æ•°æ®æŒ‰ sufficiencyÃ—sparsity èšåˆ
    - prepare_sampling_features_space: å…ˆç©ºé—´èšåˆ â†’ å† sufficiencyÃ—sparsity èšåˆ
    
    æ³¨æ„ï¼šæ­¤å‡½æ•°ä¾èµ–äº prepare_spatial_features è¿”å›çš„ bin ä¿¡æ¯ï¼Œé¿å…åå‘è®¡ç®—
          sufficiency (10^sufficiency_log) æ—¶çš„æµ®ç‚¹æ•°ç²¾åº¦æŸå¤±ã€‚
    
    Args:
        bins_intervals: (sparsity_bins_edges, suff_to_bin) - ä¼ é€’ç»™ prepare_spatial_features
        df: è®­ç»ƒæ•°æ®
        model_names: æ¨¡å‹åç§°åˆ—è¡¨
        split_by: 'grid' or 'station'
        resolution: [lon_bins, lat_bins], é»˜è®¤[10, 10]
        use_seeds: å…¼å®¹å‚æ•°ï¼ˆæœªä½¿ç”¨ï¼‰
        clip: RÂ² æˆªæ–­èŒƒå›´ [min, max]ï¼Œä¼ é€’ç»™ prepare_spatial_features
    
    Returns:
        {model_name: DataFrame with columns [sufficiency_log, sparsity, longitude, latitude, r2]}
    """
    # Step 1: å…ˆè°ƒç”¨ prepare_spatial_features è·å–ç©ºé—´èšåˆæ•°æ®ï¼ˆåŒ…å« density ç‰¹å¾ï¼‰
    spatial_results = prepare_spatial_features(
        df, model_names,
        split_by=split_by,
        include_sampling_features=True,
        bins_intervals=bins_intervals,
        resolution=resolution,
        clip=clip  # ä¼ é€’æˆªæ–­å‚æ•°
    )
    
    # Step 2: å†å¯¹ç©ºé—´èšåˆç»“æœæŒ‰ sufficiencyÃ—sparsity èšåˆ
    results_dict = {}
    
    for model_name, df_spatial in spatial_results.items():
        if len(df_spatial) == 0:
            results_dict[model_name] = pd.DataFrame()
            continue
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ bin ä¿¡æ¯ï¼ˆä» prepare_spatial_features ä¼ é€’è¿‡æ¥ï¼‰
        if 'sufficiency_bin' not in df_spatial.columns or 'sparsity_bin' not in df_spatial.columns:
            raise ValueError("spatial_results must contain 'sufficiency_bin' and 'sparsity_bin' columns")
        
        # ç›´æ¥ä½¿ç”¨å·²æœ‰çš„ bin åˆ—ï¼Œé¿å…åå‘è®¡ç®—ç²¾åº¦æŸå¤±
        df_copy = df_spatial.copy()
        df_copy = df_copy.dropna(subset=['sparsity_bin', 'sufficiency_bin'])
        
        # æŒ‰ sufficiency_bin Ã— sparsity_bin èšåˆ
        results = []
        for name, group in df_copy.groupby(['sufficiency_bin', 'sparsity_bin']):
            if len(group) < 1:
                continue
            
            result = {
                'sufficiency_log': group['sufficiency_log'].mean(),
                'sparsity': group['sparsity'].mean(),
                'longitude': group['longitude'].mean(),
                'latitude': group['latitude'].mean(),
                'sufficiency_bin': name[0],  # ä¿ç•™ bin ä¿¡æ¯
                'sparsity_bin': name[1],     # ä¿ç•™ bin ä¿¡æ¯
                'r2': group['r2'].mean()  # å¯¹å·²ç»èšåˆçš„ RÂ² å†å–å¹³å‡
            }
            results.append(result)
        
        results_dict[model_name] = pd.DataFrame(results)
    
    return results_dict


def prepare_spatial_features(df, model_names, split_by='grid', include_sampling_features=True, bins_intervals=None, resolution=None, outlier_threshold=None, clip=None):
    """
    å‡†å¤‡æµ‹è¯•æ•°æ®ï¼šæŒ‰spatialæ–¹å¼èšåˆï¼Œå¯é€‰åœ°åŒ…å«samplingç‰¹å¾
    
    Args:
        df: æµ‹è¯•æ•°æ®
        model_names: æ¨¡å‹åç§°åˆ—è¡¨
        split_by: 'grid' or 'station'
        include_sampling_features: æ˜¯å¦åŒ…å«sufficiency_logå’Œsparsityç‰¹å¾
        bins_intervals: å¦‚æœinclude_sampling_features=Trueï¼Œéœ€è¦æä¾›binè¾¹ç•Œ
        resolution: [lon_bins, lat_bins], é»˜è®¤[10, 10]
        outlier_threshold: RÂ² å¼‚å¸¸å€¼å¤„ç†ï¼ˆå·²åºŸå¼ƒï¼Œå»ºè®®ä½¿ç”¨ clipï¼‰
                          - æ•°å­—ï¼ˆå¦‚ -0.5ï¼‰ï¼šå°† RÂ² <= threshold çš„å€¼æ›¿æ¢ä¸º threshold
                          - å­—ç¬¦ä¸²ï¼ˆå¦‚ "0_remove", "-0.5_remove"ï¼‰ï¼šç§»é™¤ RÂ² <= threshold çš„æ•°æ®ç‚¹
                          - Noneï¼šä¸è¿›è¡Œä»»ä½•å¤„ç†
        clip: RÂ² æˆªæ–­èŒƒå›´ [min, max]ï¼Œåœ¨è®¡ç®— RÂ² åç«‹å³æˆªæ–­
              ä¾‹å¦‚ [-0.5, 1.0] è¡¨ç¤ºå°† RÂ² é™åˆ¶åœ¨ [-0.5, 1.0] èŒƒå›´å†…
              None è¡¨ç¤ºä¸æˆªæ–­
    
    Returns:
        {model_name: DataFrame with columns [longitude, latitude, (sparsity, sufficiency_log), r2]}
    """
    if resolution is None:
        resolution = [10, 10]
    
    df_copy = df.copy()
    
    if split_by == 'grid':
        df_copy['longitude_bin'] = pd.cut(df_copy['longitude'], bins=resolution[0], labels=False)
        df_copy['latitude_bin'] = pd.cut(df_copy['latitude'], bins=resolution[1], labels=False)
        group_cols = ['longitude_bin', 'latitude_bin']
    else:  # station
        if 'Site_number' in df_copy.columns:
            df_copy['station_id'] = df_copy['Site_number']
        else:
            df_copy['station_id'] = df_copy.groupby(['longitude', 'latitude']).ngroup()
        group_cols = ['station_id']
    
    df_copy = df_copy.dropna(subset=group_cols)
    
    # å¦‚æœéœ€è¦samplingç‰¹å¾ï¼Œåˆ†é…bins
    if include_sampling_features and bins_intervals is not None:
        sparsity_bins_edges, suff_to_bin = bins_intervals
        df_copy['sparsity_bin'] = pd.cut(df_copy['sparsity'], bins=sparsity_bins_edges, labels=False)
        df_copy['sufficiency_bin'] = df_copy['sufficiency'].map(suff_to_bin)
        group_cols = group_cols + ['sufficiency_bin']
    
    results_dict = {}
    
    for model_name in model_names:
        model_col = f'predicted_{model_name}'
        if model_col not in df_copy.columns:
            continue
        
        results = []
        
        for name, group in df_copy.groupby(group_cols):
            if len(group) < 3:
                continue
            
            observed = group['observed'].values
            predicted = group[model_col].values
            valid_mask = ~(np.isnan(observed) | np.isnan(predicted) | 
                          np.isinf(observed) | np.isinf(predicted))
            
            if valid_mask.sum() < 5:
                continue
            
            from sklearn.metrics import r2_score
            r2 = r2_score(observed[valid_mask], predicted[valid_mask])
            
            # ğŸ”§ Clip RÂ² å€¼ï¼ˆåœ¨æºæ•°æ®é˜¶æ®µï¼Œä¼˜å…ˆäº outlier_thresholdï¼‰
            if clip is not None and len(clip) == 2:
                r2 = np.clip(r2, clip[0], clip[1])
            
            # Handle outlier threshold (legacy, å»ºè®®ä½¿ç”¨ clip)
            if outlier_threshold is not None:
                # Check if it's a string with "_remove" suffix
                if isinstance(outlier_threshold, str) and outlier_threshold.endswith('_remove'):
                    # Extract threshold value and remove if RÂ² <= threshold
                    threshold_str = outlier_threshold.replace('_remove', '')
                    try:
                        threshold_val = float(threshold_str)
                        if r2 <= threshold_val:
                            continue  # Skip this data point (remove)
                    except ValueError:
                        pass  # Invalid format, ignore
                elif isinstance(outlier_threshold, (int, float)):
                    # Numeric threshold: replace value
                    if r2 <= outlier_threshold:
                        r2 = outlier_threshold
            
            result = {
                'longitude': group['longitude'].mean(),
                'latitude': group['latitude'].mean(),
                'r2': r2
            }
            
            if include_sampling_features:
                result['sparsity'] = group['sparsity'].mean()
                result['sufficiency_log'] = np.log10(group['sufficiency'].mean())
                # ä¿ç•™ bin ä¿¡æ¯ï¼ˆç”¨äºåç»­èšåˆï¼Œé¿å…åå‘è®¡ç®—ç²¾åº¦æŸå¤±ï¼‰
                result['sparsity_bin'] = group['sparsity_bin'].iloc[0]  # bin åœ¨ç»„å†…æ˜¯ç›¸åŒçš„
                result['sufficiency_bin'] = group['sufficiency_bin'].iloc[0]
            
            results.append(result)
        
        results_dict[model_name] = pd.DataFrame(results)
    
    return results_dict
  

def _calculate_spatial_r2(df, model_name, resolution=None):
    """è®¡ç®—ç©ºé—´RÂ²æ•°æ®çš„è¾…åŠ©å‡½æ•°"""
    if resolution is None:
        resolution = [10, 10]
    
    model_col = f'predicted_{model_name}'
    if model_col not in df.columns:
        return pd.DataFrame()
    
    df_copy = df.copy()
    df_copy['longitude_bin'] = pd.cut(df_copy['longitude'], bins=resolution[0], labels=False)
    df_copy['latitude_bin'] = pd.cut(df_copy['latitude'], bins=resolution[1], labels=False)
    
    results = []
    for name, group in df_copy.groupby(['longitude_bin', 'latitude_bin']):
        if len(group) < 10 or pd.isna(name[0]) or pd.isna(name[1]):
            continue
        
        observed = group['observed'].values
        predicted = group[model_col].values
        valid_mask = ~(np.isnan(observed) | np.isnan(predicted) | 
                      np.isinf(observed) | np.isinf(predicted))
        
        if valid_mask.sum() >= 5:
            from sklearn.metrics import r2_score
            r2 = r2_score(observed[valid_mask], predicted[valid_mask])
            results.append({
                'longitude': group['longitude'].mean(),
                'latitude': group['latitude'].mean(),
                'r2': r2
            })
    
    return pd.DataFrame(results)

def _evaluate_trained_models(trained_models, test_data_dict, model_names, model_list, feature_cols, metric):
    """
    ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨æµ‹è¯•æ•°æ®ä¸Šè¯„ä¼°
    
    Args:
        trained_models: {model_name: {model_type: {'model': ..., 'scaler': ...}}}
        test_data_dict: {model_name: test_dataframe}
        model_names: æ¨¡å‹åç§°åˆ—è¡¨
        model_list: è¦è¯„ä¼°çš„æ¨¡å‹ç±»å‹åˆ—è¡¨
        feature_cols: ç‰¹å¾åˆ—ååˆ—è¡¨
        metric: 'r2' or 'correlation'
    
    Returns:
        results: {model_name: {model_type: score}}
    """
    from sklearn.metrics import r2_score
    from scipy.stats import pearsonr
    
    results = {}
    
    for model_name in model_names:
        model_results = {}
        model_dict = trained_models.get(model_name)
        model_test_data = test_data_dict.get(model_name, pd.DataFrame())
        
        if model_dict is None or len(model_test_data) < 3:
            for model_type in model_list:
                model_results[model_type] = -999
            results[model_name] = model_results
            continue
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        X_test_normal = model_test_data[feature_cols].values
        y_test = model_test_data['r2'].values
        
        # ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹å¹¶è¯„ä¼°
        for model_type in model_list:
            model_info = model_dict.get(model_type)
            if model_info is None:
                model_results[model_type] = -999
                continue
            
            model_obj = model_info['model']
            scaler = model_info['scaler']
            
            # é¢„æµ‹å¹¶è¯„ä¼°
            try:
                # é€‰æ‹©æµ‹è¯•æ•°æ®
                if model_type == 'interpolation':
                    # Interpolation: ä½¿ç”¨ lon/lat
                    X_test = model_test_data[['longitude', 'latitude']].values
                elif model_type in ['two_stage', 'two_stage_1', 'two_stage_2']:
                    # Two-Stage: éœ€è¦æ‰€æœ‰ 4 ä¸ªç‰¹å¾ [lon, lat, suff_log, sparsity]
                    required_cols = ['longitude', 'latitude', 'sufficiency_log', 'sparsity']
                    if not all(col in model_test_data.columns for col in required_cols):
                        model_results[model_type] = -999
                        continue
                    X_test = model_test_data[required_cols].values
                else:
                    # å…¶ä»–æ¨¡å‹ï¼šä½¿ç”¨æŒ‡å®šç‰¹å¾
                    X_test = X_test_normal
                
                # ç»Ÿä¸€è°ƒç”¨ .predict() æ¥å£
                if scaler is not None:
                    X_test_scaled = scaler.transform(X_test)
                    y_pred = model_obj.predict(X_test_scaled)
                else:
                    # ç‰¹æ®Šå¤„ç†ï¼štwo_stage ç³»åˆ—
                    if model_type == 'two_stage_1' and hasattr(model_obj, 'predict_stage1_only'):
                        y_pred = model_obj.predict_stage1_only(X_test)
                    elif model_type == 'two_stage_2' and hasattr(model_obj, 'predict_stage2_only'):
                        y_pred = model_obj.predict_stage2_only(X_test)
                    else:
                        y_pred = model_obj.predict(X_test)
                
                # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
                if metric == 'correlation':
                    score, _ = pearsonr(y_test, y_pred)
                else:
                    score = r2_score(y_test, y_pred)
                
                model_results[model_type] = score
            except Exception as e:
                model_results[model_type] = -999
        
        results[model_name] = model_results
    
    return results


def _print_performance_table(results, model_list, metric='r2'):
    """æ‰“å°æ€§èƒ½è¡¨æ ¼çš„è¾…åŠ©å‡½æ•°"""
    metric_name = 'RÂ²' if metric == 'r2' else 'Correlation'
    
    def format_score(score):
        return f"{score:.4f}" if score != -999 else "FAILED"
    
    # æ¨¡å‹åç§°æ˜ å°„ï¼ˆç”¨äºè¡¨å¤´æ˜¾ç¤ºï¼‰
    model_display_names = {
        'linear': 'Linear', 'ridge': 'Ridge', 'elasticnet': 'ElasticNet',
        'rf': 'RF', 'gbr': 'GBR', 'lightgbm': 'LightGBM', 'svm': 'SVM',
        'gam_free': 'GAM_Free', 'gam_monotonic': 'GAM_Mono',
        'gam_without_interaction': 'GAM_NoInt', 'gam_mono_noint': 'GAM_MonoNoInt',
        'interpolation': 'Interp', 'two_stage': 'TwoStage', 'two_stage_1': 'Stage1', 
        'two_stage_2': 'Stage2_Resid'
    }
    
    # è¡¨å¤´
    headers = ['Model'] + [model_display_names.get(m, m) for m in model_list]
    col_width = 12
    table_width = 15 + col_width * len(model_list)
    
    print(f"\n{'='*table_width}")
    print(f"Model Performance Comparison ({metric_name})")
    print(f"{'='*table_width}")
    print(f"{headers[0]:<15} " + " ".join(f"{h:<{col_width}}" for h in headers[1:]))
    print("-" * table_width)
    
    # æ•°æ®è¡Œ
    for model_name, result in results.items():
        row = [f"{model_name:<15}"]
        for method in model_list:
            row.append(f"{format_score(result.get(method, -999)):<{col_width}}")
        print(" ".join(row))
    
    # è®¡ç®—å¹³å‡è¡Œ
    averages = {}
    for method in model_list:
        valid_scores = [result[method] for result in results.values() if method in result and result[method] != -999]
        averages[method] = np.mean(valid_scores) if valid_scores else -999
    
    # æ‰“å°å¹³å‡è¡Œ
    print("-" * table_width)
    avg_row = [f"{'AVERAGE':<15}"]
    for method in model_list:
        avg_row.append(f"{format_score(averages[method]):<{col_width}}")
    print(" ".join(avg_row))
    
    print("="*table_width)
    print("Note: 'FAILED' indicates model training failed")

def compare_gam_types(df, sparsity_bins, target_model='Ours', use_seeds=True, 
                     spline=14, lam=5, remove_outliers=True):
    """
    æ¯”è¾ƒä¸åŒå•è°ƒå‡½æ•°ç±»å‹çš„GAMæ¨¡å‹æ•ˆæœ
    
    Args:
        df: æ•°æ®æ¡†
        sparsity_bins: sparsityåˆ†binæ•°é‡
        target_model: ç›®æ ‡æ¨¡å‹åç§°
        use_seeds: æ˜¯å¦ä½¿ç”¨seedsèšåˆ
        spline: æ ·æ¡å‡½æ•°èŠ‚ç‚¹æ•°
        lam: æ­£åˆ™åŒ–å‚æ•°
        remove_outliers: æ˜¯å¦ç§»é™¤å¼‚å¸¸å€¼
        
    Returns:
        dict: å„ç§GAMç±»å‹çš„æ‹Ÿåˆç»“æœ
    """
    print(f"ğŸ”„ Comparing different monotonic GAM types for {target_model}...")
    
    gam_types = ['spline', 'linear', 'mixed', 'polynomial']
    results = {}
    
    for gam_type in gam_types:
        print(f"\nğŸ“Š Testing {gam_type} GAM...")
        
        try:
            fitted_models = fit_gam_models(
                df, sparsity_bins, use_seeds=use_seeds, 
                spline=spline, lam=lam, verbose=False,
                remove_outliers=remove_outliers, gam_type=gam_type
            )
            
            if target_model in fitted_models:
                model_info = fitted_models[target_model]
                r2_val = model_info['r2_score']
                results[gam_type] = {
                    'r2_score': r2_val,
                    'model_info': model_info,
                    'status': 'success'
                }
                print(f"   âœ… {gam_type}: RÂ² = {r2_val:.4f}")
            else:
                results[gam_type] = {'status': 'failed', 'error': 'Model not found'}
                print(f"   âŒ {gam_type}: Failed - Model not found")
                
        except Exception as e:
            results[gam_type] = {'status': 'failed', 'error': str(e)}
            print(f"   âŒ {gam_type}: Failed - {str(e)}")
    
    # æ˜¾ç¤ºæ’å
    successful_results = {k: v for k, v in results.items() if v['status'] == 'success'}
    if successful_results:
        print(f"\nğŸ† GAM Type Rankings (by RÂ²):")
        sorted_results = sorted(successful_results.items(), 
                              key=lambda x: x[1]['r2_score'], reverse=True)
        
        for i, (gam_type, info) in enumerate(sorted_results, 1):
            r2_val = info['r2_score']
            print(f"   {i}. {gam_type.upper():<12}: RÂ² = {r2_val:.4f}")
    
    return results


# =============================================================================
# Plotting Helper Functions
# =============================================================================

def calculate_station_r2(df_analysis, model_name, sufficiency):
    """
    Calculate RÂ² for each station (longitude, latitude).
    
    Args:
        df_analysis: DataFrame with observed and predicted values
        model_name: Name of the model (e.g., 'lightgbm')
        sufficiency: Sufficiency value to filter data
    
    Returns:
        DataFrame with columns: longitude, latitude, r2
    """
    df_plot = df_analysis[df_analysis['sufficiency'] == sufficiency].copy()
    
    predicted_col = f'predicted_{model_name}'
    if predicted_col not in df_plot.columns:
        print(f"Warning: Column '{predicted_col}' not found in df_analysis")
        return pd.DataFrame(columns=['longitude', 'latitude', 'r2'])
    
    station_r2_list = []
    for (lon, lat), group in df_plot.groupby(['longitude', 'latitude']):
        if len(group) > 1:
            r2 = r2_score(group['observed'], group[predicted_col])
            station_r2_list.append({'longitude': lon, 'latitude': lat, 'r2': r2})
    
    return pd.DataFrame(station_r2_list)


def plot_observation_points(ax, station_data, cmap, vmin, vmax, marker_size=15):
    """
    Plot station observation points on the map.
    
    Args:
        ax: matplotlib Axes object
        station_data: DataFrame with longitude, latitude, r2 columns
        cmap: matplotlib colormap
        vmin: Minimum value for colormap
        vmax: Maximum value for colormap
        marker_size: Size of scatter points
    
    Returns:
        scatter: matplotlib scatter object
    """
    if len(station_data) == 0:
        print("Warning: No valid stations to plot")
        return None
    
    scatter = ax.scatter(
        station_data['longitude'],
        station_data['latitude'],
        c=station_data['r2'],
        cmap=cmap,
        vmin=vmin,
        vmax=vmax,
        s=marker_size,
        alpha=0.8,
        edgecolors='black',
        linewidths=0.5
    )
    return scatter


def plot_grid_average(ax, df_analysis, model_name, sufficiency, cmap, vmin, vmax, grid_shape=(15, 20)):
    """
    Plot grid-averaged accuracy by calculating RÂ² from all points within each grid cell.
    
    Args:
        ax: matplotlib Axes object
        df_analysis: DataFrame with observed, predicted, longitude, latitude columns
        model_name: Name of the model
        sufficiency: Sufficiency value to filter data
        cmap: matplotlib colormap
        vmin, vmax: Colormap range
        grid_shape: Tuple (n_lat_bins, n_lon_bins)
    
    Returns:
        pcolormesh: matplotlib pcolormesh object
    """
    df_plot = df_analysis[df_analysis['sufficiency'] == sufficiency].copy()
    
    predicted_col = f'predicted_{model_name}'
    if predicted_col not in df_plot.columns or len(df_plot) == 0:
        print(f"Warning: No data for {model_name}")
        return None
    
    n_lat_bins, n_lon_bins = grid_shape
    
    lon_min, lon_max = df_plot['longitude'].min(), df_plot['longitude'].max()
    lat_min, lat_max = df_plot['latitude'].min(), df_plot['latitude'].max()
    
    lon_bins = np.linspace(lon_min, lon_max, n_lon_bins + 1)
    lat_bins = np.linspace(lat_min, lat_max, n_lat_bins + 1)
    
    df_plot['lon_bin'] = np.digitize(df_plot['longitude'], lon_bins) - 1
    df_plot['lat_bin'] = np.digitize(df_plot['latitude'], lat_bins) - 1
    df_plot['lon_bin'] = df_plot['lon_bin'].clip(0, n_lon_bins - 1)
    df_plot['lat_bin'] = df_plot['lat_bin'].clip(0, n_lat_bins - 1)
    
    grid_r2 = np.full((n_lat_bins, n_lon_bins), np.nan)
    
    for (lat_bin, lon_bin), group in df_plot.groupby(['lat_bin', 'lon_bin']):
        if len(group) > 1:
            r2 = r2_score(group['observed'], group[predicted_col])
            grid_r2[int(lat_bin), int(lon_bin)] = r2
    
    mesh = ax.pcolormesh(lon_bins, lat_bins, grid_r2, cmap=cmap, vmin=vmin, vmax=vmax, alpha=0.8, shading='flat')
    return mesh


def plot_anomaly_stations(ax, station_data, marker_size=20, marker_color='red'):
    """
    Plot anomaly stations (RÂ² < 0).
    
    Args:
        ax: matplotlib Axes object
        station_data: DataFrame with longitude, latitude, r2 columns
        marker_size: Size of scatter points
        marker_color: Color for anomaly markers
    
    Returns:
        scatter: matplotlib scatter object
    """
    anomaly_data = station_data[station_data['r2'] < 0].copy()
    
    if len(anomaly_data) == 0:
        print("No anomaly stations (RÂ² < 0) found")
        return None
    
    print(f"Found {len(anomaly_data)} anomaly stations with RÂ² < 0")
    
    scatter = ax.scatter(
        anomaly_data['longitude'],
        anomaly_data['latitude'],
        c=marker_color,
        s=marker_size,
        alpha=0.8,
        edgecolors='black',
        linewidths=0.8,
        marker='x',
        label=f'Anomaly (RÂ²<0): {len(anomaly_data)}'
    )
    ax.legend(loc='upper right', fontsize=8)
    return scatter


def plot_station_points(ax, df_data, color='#2C2C2C', alpha=0.3, size=6, marker=None):
    """
    Plot station distribution points.
    
    Args:
        ax: matplotlib Axes object
        df_data: DataFrame with longitude and latitude columns
        color: Point color
        alpha: Transparency
        size: Point size
        marker: Marker style
    
    Returns:
        scatter: scatter plot object
    """
    unique_locs = df_data.groupby(['longitude', 'latitude']).size()
    data_lons = unique_locs.index.get_level_values('longitude')
    data_lats = unique_locs.index.get_level_values('latitude')
    
    scatter = ax.scatter(data_lons, data_lats, c=color, alpha=alpha, s=size, 
                        edgecolors='none', marker='o' if marker is None else marker)
    return scatter


def plot_multipolygon_edges(ax, gdf, edge_color='black', linewidth=1, alpha=1):
    """
    Plot polygon edges from a GeoDataFrame.
    
    Args:
        ax: matplotlib Axes object
        gdf: GeoDataFrame with geometry
        edge_color: Edge line color
        linewidth: Edge line width
        alpha: Transparency
    """
    for geometry in gdf.geometry:
        if geometry.geom_type == 'Polygon':
            x, y = geometry.exterior.coords.xy
            ax.plot(x, y, color=edge_color, linewidth=linewidth, alpha=alpha)
        elif geometry.geom_type == 'MultiPolygon':
            for polygon in geometry.geoms:
                x, y = polygon.exterior.coords.xy
                ax.plot(x, y, color=edge_color, linewidth=linewidth, alpha=alpha)


def plot_sparsity_quantile_contour(ax, ds_sparsity, count_number=3, levels=None, colors=None, 
                                   linewidth=2.5, alpha=1, show_labels=True, label_format=None, 
                                   line_styles=None):
    """
    Plot sparsity density quantile contours.
    
    Args:
        ax: matplotlib Axes object
        ds_sparsity: xarray Dataset with sparsity values
        count_number: Number of quantile lines (used only if levels=None)
        levels: List of specific level values (overrides quantiles)
        colors: List of colors for each contour line
        linewidth: Line width (single value or list)
        alpha: Transparency
        show_labels: Whether to show contour labels
        label_format: Label format string
        line_styles: List of line styles
    
    Returns:
        contours: list of contour objects
    """
    import matplotlib.pyplot as plt
    
    lons = ds_sparsity.longitude.values
    lats = ds_sparsity.latitude.values
    sparsity_grid = ds_sparsity['sparsity'].values
    
    lon_grid, lat_grid = np.meshgrid(lons, lats)
    
    if levels is not None:
        quantile_levels = np.array(levels)
        count_number = len(levels)
        valid_values = sparsity_grid[~np.isnan(sparsity_grid)]
        percentiles = [np.sum(valid_values <= level) / len(valid_values) * 100 for level in levels] if len(valid_values) > 0 else [0] * len(levels)
    else:
        valid_values = sparsity_grid[~np.isnan(sparsity_grid)]
        if len(valid_values) == 0:
            print("Warning: No valid sparsity values found")
            return []
        percentiles = np.linspace(0, 100, count_number + 2)[1:-1]
        quantile_levels = np.percentile(valid_values, percentiles)
        percentiles = percentiles[::-1]
        quantile_levels = quantile_levels[::-1]
    
    if colors is None or len(colors) != count_number:
        colors = plt.cm.Reds(np.linspace(0.4, 0.9, count_number))
    
    if line_styles is None or len(line_styles) != count_number:
        line_styles = ['-'] * count_number
    
    if isinstance(linewidth, (list, tuple, np.ndarray)):
        linewidths = list(linewidth) if len(linewidth) == count_number else [linewidth[0]] * count_number
    else:
        linewidths = [linewidth] * count_number
    
    contours = []
    for i, (level, color, percentile, line_style, lw) in enumerate(zip(quantile_levels, colors, percentiles, line_styles, linewidths)):
        contour = ax.contour(lon_grid, lat_grid, sparsity_grid, levels=[level], colors=[color], linewidths=lw, alpha=alpha, linestyles=line_style)
        
        if show_labels:
            if label_format is not None:
                label_text = label_format.format(percentile=percentile, level=level, index=i+1)
            else:
                label_text = f'{level:.2f}' if levels is not None else f'P{percentile:.0f}'
            ax.clabel(contour, inline=True, fontsize=8, fmt=label_text)
        
        contours.append(contour)
    
    return contours


def _get_model_info(fitted_models, model_name, model_type=None):
    """
    Extract model_info from fitted_models dictionary.
    
    Args:
        fitted_models: Dictionary of fitted models
        model_name: Model name
        model_type: Model type (optional, for API compatibility)
    
    Returns:
        model_info: Dictionary with 'model', 'scaler', 'full_features', etc.
    """
    if model_name not in fitted_models:
        raise ValueError(f"Model '{model_name}' not found in fitted_models")
    return fitted_models[model_name]


def predict_accuracy_grid(model_info, prediction_input):
    """
    Predict accuracy values using the model.
    
    Args:
        model_info: Dictionary containing model and scaler
        prediction_input: Input data for prediction
    
    Returns:
        accuracy_pred: Predicted accuracy values
    """
    if isinstance(model_info, dict):
        model = model_info['model']
        scaler = model_info.get('scaler')
        model_type = model_info.get('model_type')
    else:
        model = model_info
        scaler = None
        model_type = None
    
    if scaler is not None:
        prediction_input = scaler.transform(prediction_input)
    
    if model_type == 'two_stage_1' and hasattr(model, 'predict_stage1_only'):
        return model.predict_stage1_only(prediction_input)
    elif model_type == 'two_stage_2' and hasattr(model, 'predict_stage2_only'):
        return model.predict_stage2_only(prediction_input)
    else:
        return model.predict(prediction_input)


def _prepare_accuracy_grid(fitted_models, model_name, ds_sparsity, sufficiency_value, model_type=None):
    """
    Prepare accuracy grid for a given sufficiency value.
    
    Args:
        fitted_models: Dictionary of fitted models
        model_name: Model name
        ds_sparsity: xarray Dataset with sparsity values
        sufficiency_value: Fixed sufficiency value
        model_type: Model type (optional)
    
    Returns:
        lon_grid, lat_grid, accuracy_grid: Coordinate grids and predicted accuracy
    """
    model_info = _get_model_info(fitted_models, model_name, model_type)
    
    lons = ds_sparsity.longitude.values
    lats = ds_sparsity.latitude.values
    sparsity_grid = ds_sparsity['sparsity'].values
    
    lon_grid, lat_grid = np.meshgrid(lons, lats)
    
    full_features = model_info.get('full_features', 'Density')
    sparsity_values = sparsity_grid.ravel()
    
    if isinstance(full_features, list):
        lon_values = lon_grid.ravel()
        lat_values = lat_grid.ravel()
        sufficiency_values = np.full(len(sparsity_values), np.log10(sufficiency_value))
        
        feature_map = {
            'longitude': lon_values,
            'latitude': lat_values,
            'sufficiency_log': sufficiency_values,
            'sparsity': sparsity_values
        }
        model_input = np.column_stack([feature_map[feat] for feat in full_features])
        
    elif full_features == 'Spatial':
        lon_values = lon_grid.ravel()
        lat_values = lat_grid.ravel()
        model_input = np.column_stack([lon_values, lat_values])
        
    elif full_features == 'Full':
        lon_values = lon_grid.ravel()
        lat_values = lat_grid.ravel()
        sufficiency_log = np.full(len(sparsity_values), np.log10(sufficiency_value))
        model_input = np.column_stack([lon_values, lat_values, sufficiency_log, sparsity_values])
        
    else:  # Density
        sufficiency_log = np.full(len(sparsity_values), np.log10(sufficiency_value))
        model_input = np.column_stack([sufficiency_log, sparsity_values])
    
    valid_mask = ~np.isnan(sparsity_values)
    
    if np.sum(valid_mask) == 0:
        accuracy_grid = np.full(sparsity_grid.shape, np.nan)
    else:
        valid_model_input = model_input[valid_mask]
        accuracy_pred_valid = predict_accuracy_grid(model_info, valid_model_input)
        
        accuracy_pred = np.full(len(sparsity_values), np.nan)
        accuracy_pred[valid_mask] = accuracy_pred_valid
        accuracy_grid = accuracy_pred.reshape(sparsity_grid.shape)
    
    return lon_grid, lat_grid, accuracy_grid


def plot_accuracy_hexbin(ax, fitted_models, model_name, ds_sparsity, sufficiency_value,
                         gridsize=30, cmap='viridis', vmin=None, vmax=None, alpha=1,
                         add_colorbar=True, colorbar_label='Accuracy', threshold=None,
                         levels=None, contour_colors='black', contour_linewidths=1,
                         contour_alpha=1.0, contour_linestyles='solid', show_labels=True, model_type=None):
    """
    Plot accuracy hexbin grid.
    
    Args:
        ax: matplotlib Axes object
        fitted_models: Dictionary of fitted models
        model_name: Model name
        ds_sparsity: xarray Dataset with sparsity values
        sufficiency_value: Fixed sufficiency value
        gridsize: Hexbin grid size
        cmap: Colormap
        vmin, vmax: Colormap range
        alpha: Transparency
        add_colorbar: Whether to add colorbar
        colorbar_label: Colorbar label
        threshold: Values below this are shown in gray
        levels: Contour levels
        contour_colors: Contour line colors
        contour_linewidths: Contour line widths
        contour_alpha: Contour transparency
        contour_linestyles: Contour line styles
        show_labels: Whether to show contour labels
        model_type: Model type
    
    Returns:
        hexbin plot object(s)
    """
    import matplotlib.cm as cm
    
    lon_grid, lat_grid, accuracy_grid = _prepare_accuracy_grid(
        fitted_models, model_name, ds_sparsity, sufficiency_value, model_type=model_type)
    
    lon_points = lon_grid.ravel()
    lat_points = lat_grid.ravel()
    accuracy_points = accuracy_grid.ravel()
    
    valid_mask = ~np.isnan(accuracy_points)
    lon_points = lon_points[valid_mask]
    lat_points = lat_points[valid_mask]
    accuracy_points = accuracy_points[valid_mask]
    
    if len(accuracy_points) == 0:
        print(f"Warning: No valid accuracy values for {model_name}")
        return None
    
    if threshold is None and vmin is not None:
        threshold = vmin
    
    if isinstance(cmap, str):
        cmap_obj = cm.get_cmap(cmap).copy()
    else:
        cmap_obj = cmap.copy()
    cmap_obj.set_under('lightgray')
    
    if threshold is not None:
        accuracy_points_modified = accuracy_points.copy()
        below_threshold_mask = accuracy_points < threshold
        if np.any(below_threshold_mask):
            accuracy_points_modified[below_threshold_mask] = threshold - 1e-10
        accuracy_points = accuracy_points_modified
    
    hexbin_plot = ax.hexbin(lon_points, lat_points, C=accuracy_points,
                           gridsize=gridsize, cmap=cmap_obj, vmin=vmin, vmax=vmax,
                           alpha=alpha, reduce_C_function=np.mean)
    
    contour_plot = None
    if levels is not None:
        if isinstance(contour_linewidths, (list, tuple, np.ndarray)):
            linewidths_to_use = contour_linewidths if len(contour_linewidths) == len(levels) else contour_linewidths[0]
        else:
            linewidths_to_use = contour_linewidths
        
        contour_plot = ax.contour(lon_grid, lat_grid, accuracy_grid,
                                 levels=levels, colors=contour_colors,
                                 linewidths=linewidths_to_use,
                                 alpha=contour_alpha, linestyles=contour_linestyles)
        if show_labels:
            ax.clabel(contour_plot, inline=True, fontsize=8, fmt='%.1f')
    
    if add_colorbar:
        import matplotlib.pyplot as plt
        cbar = plt.colorbar(hexbin_plot, ax=ax, label=colorbar_label, extend='min')
        if contour_plot is not None:
            return hexbin_plot, cbar, contour_plot
        return hexbin_plot, cbar
    
    if contour_plot is not None:
        return hexbin_plot, contour_plot
    return hexbin_plot
